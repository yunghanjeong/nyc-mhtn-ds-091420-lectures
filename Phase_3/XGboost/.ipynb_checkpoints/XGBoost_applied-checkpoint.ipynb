{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an implementation of Gradient Boosting Machines (GBM) and is used for supervised learning. XGBoost is an open-sourced machine learning library available in Python, R, Julia, Java, C++, Scala. The features that standout are:\n",
    "\n",
    "- Speed\n",
    "- Awareness of sparse data\n",
    "- Implementation on single, distributed systems and out-of-core computation\n",
    "- Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting carries the principle of Gradient Descent and Boosting to supervised learning. Gradient Boosted Models (GBM’s) are trees built sequentially, in series. In GBM’s, we take the weighted sum of multiple models.\n",
    "\n",
    "- Each new model uses Gradient Descent optimization to update/ make corrections to the weights to be learned by the model to reach a local minima of the cost function.\n",
    "- The vector of weights assigned to each model is not derived from the misclassifications of the previous model and the resulting increased weights assigned to misclassifications, but is derived from the weights optimized by Gradient Descent to minimize the cost function. The result of Gradient Descent is the same function of the model as the beginning, just with better parameters.\n",
    "- Gradient Boosting adds a new function to the existing function in each step to predict the output. The result of Gradient Boosting is an altogether different function from the beginning, because the result is the addition of multiple functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "              validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.5, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           #scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=1, reg_lambda=1, scale_pos_weight=1, subsample=0.5,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.780269\n",
      "F1: 0.662069\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=4,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660379</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.661152</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643254</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.644139</td>\n",
       "      <td>0.010106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.630917</td>\n",
       "      <td>0.009163</td>\n",
       "      <td>0.633101</td>\n",
       "      <td>0.010630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.619612</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>0.622553</td>\n",
       "      <td>0.011074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.614089</td>\n",
       "      <td>0.011281</td>\n",
       "      <td>0.617477</td>\n",
       "      <td>0.010654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.361463</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.426287</td>\n",
       "      <td>0.039331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.361390</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.426189</td>\n",
       "      <td>0.039451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.360983</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>0.426244</td>\n",
       "      <td>0.039909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.360753</td>\n",
       "      <td>0.011790</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>0.039985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.360338</td>\n",
       "      <td>0.011897</td>\n",
       "      <td>0.426116</td>\n",
       "      <td>0.040178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.660379           0.000837           0.661152   \n",
       "1              0.643254           0.008338           0.644139   \n",
       "2              0.630917           0.009163           0.633101   \n",
       "3              0.619612           0.012002           0.622553   \n",
       "4              0.614089           0.011281           0.617477   \n",
       "..                  ...                ...                ...   \n",
       "148            0.361463           0.011803           0.426287   \n",
       "149            0.361390           0.011806           0.426189   \n",
       "150            0.360983           0.011734           0.426244   \n",
       "151            0.360753           0.011790           0.426230   \n",
       "152            0.360338           0.011897           0.426116   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001650  \n",
       "1            0.010106  \n",
       "2            0.010630  \n",
       "3            0.011074  \n",
       "4            0.010654  \n",
       "..                ...  \n",
       "148          0.039331  \n",
       "149          0.039451  \n",
       "150          0.039909  \n",
       "151          0.039985  \n",
       "152          0.040178  \n",
       "\n",
       "[153 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEWCAYAAABfdFHAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAwI0lEQVR4nO3deXhV5bn38e+PQUqJishwqICIUEGmCFakL5WgRVSw1KG2Kq2gluPbWueBIy8WtSpSsdiWo3UsVetUJ454nLvVOhRFIwgYaDUtKIrYOoRBknC/f+xFDCEhQVnZSfh9ritX1n7Ws9a6103Yd9azVvajiMDMzCwtzXIdgJmZNW0uNGZmlioXGjMzS5ULjZmZpcqFxszMUuVCY2ZmqXKhMWsgJF0k6aZcx2G2vcl/R2NNgaRioBNQXqn56xHx7pfc56kR8eSXi67xkTQV6BkR43IdizV+vqKxpuTIiMir9PWFi8z2IKlFLo//RTXWuK3hcqGxJk3SrpJulrRS0juSfiGpebJub0lPS/pQ0mpJd0hqm6y7DegG/I+kEkkXSCqQtKLK/oslfTtZnirpT5Jul/QJMH5rx68m1qmSbk+Wu0sKSRMkLZf0b0mnSfqGpAWSPpL020rbjpf0vKTfSPpY0puSDqm0/muS5kj6l6S/SfpxleNWjvs04CLg+8m5v570myBpiaRPJb0l6T8r7aNA0gpJ50palZzvhErrW0uaIekfSXx/kdQ6WXegpBeSc3pdUsEX+Ke2BsyFxpq62UAZ0BPYDzgUODVZJ+BK4GtAH6ArMBUgIn4I/JPPr5Km1/F4Y4E/AW2BO2o5fl0MAXoB3wdmApOBbwN9geMkDa/S9y2gPfBz4H5J7ZJ1dwIrknM9FriiciGqEvfNwBXA3cm5D0z6rALGALsAE4BfSRpUaR//AewK7AGcAsyStFuy7mpgMPBNoB1wAbBR0h7AXOAXSft5wH2SOmxDjqyBc6GxpuTB5LfijyQ9KKkTcDhwVkSsiYhVwK+AHwBExN8i4omI+CwiPgCuAYbXvPs6eTEiHoyIjWTfkGs8fh1dFhHrI+JxYA1wZ0Ssioh3gOfIFq9NVgEzI6I0Iu4GioDRkroCw4ALk30VAjcBP6wu7ohYV10gETE3Iv4eWc8AjwPfqtSlFLg0Of4jQAmwj6RmwMnAmRHxTkSUR8QLEfEZMA54JCIeSY79BPAKcMQ25MgaOI/FWlPy3co37iUdALQEVkra1NwMWJ6s7wj8muyb5c7Jun9/yRiWV1rec2vHr6P3Ky2vq+Z1XqXX78TmT/f8g+wVzNeAf0XEp1XW7V9D3NWSdDjZK6Wvkz2PrwILK3X5MCLKKr1em8TXHvgK8Pdqdrsn8D1JR1Zqawn8ubZ4rPFwobGmbDnwGdC+yhvgJlcCAQyIiA8lfRf4baX1VR/JXEP2zRWA5F5L1SGeytvUdvztbQ9JqlRsugFzgHeBdpJ2rlRsugHvVNq26rlu9lpSK+A+4EfAQxFRKulBssOPtVkNrAf2Bl6vsm45cFtE/HiLrazJ8NCZNVkRsZLs8M4MSbtIapY8ALBpeGxnssM7HyX3Cs6vsov3gR6VXi8FviJptKSWwP8DWn2J429vHYEzJLWU9D2y950eiYjlwAvAlZK+ImkA2Xsod2xlX+8D3ZNhL4CdyJ7rB0BZcnVzaF2CSoYRbwGuSR5KaC5paFK8bgeOlDQqaf9K8mBBl20/fWuoXGisqfsR2TfJxWSHxf4EdE7WXQIMAj4me0P6/irbXgn8v+Sez3kR8THwE7L3N94he4Wzgq3b2vG3t7+SfXBgNXA5cGxEfJisOx7oTvbq5gHg58n9kJrcm3z/UNKryZXQGcA9ZM/jBLJXS3V1HtlhtpeBfwFXAc2SIjiW7FNuH5C9wjkfvzc1Kf6DTbMmQNJ4sn9cOizXsZhV5d8azMwsVS40ZmaWKg+dmZlZqnxFY2ZmqfLf0VTRtm3b6NmzZ67DaPDWrFlDmzZtch1Gg+Yc1c45qpvGkKf58+evjohqPzrIhaaKTp068corr+Q6jAYvk8lQUFCQ6zAaNOeods5R3TSGPEn6R03rPHRmZmapcqExM7NUudCYmVmqXGjMzCxVLjRmZpYqFxozM0uVC42ZmaXKhcbMzFLlQmNmZqlyoTEzs1S50JiZWapcaMzMLFUuNGZmlioXGjMzS5ULjZmZpcqFxszMUuVCY2bWxCxfvpwRI0bQp08f+vbty7XXXgvAv/71L0aOHEmvXr0YOXIk//73vwG44447yM/Pr/hq1qwZhYWFAGzYsIGJEyfy9a9/nd69e3PfffdtczyNotBIKpdUWOmre65jMjNrqFq0aMGMGTNYsmQJL730ErNmzWLx4sVMmzaNQw45hGXLlnHIIYcwbdo0AE488UQKCwspLCzktttuo3v37uTn5wNw+eWX07FjR5YuXcrixYsZPnz4NsejiNie55cKSSURkbeN24js+W3clu269egZzY67dpvi2xGd27+MGQs9E/jWOEe1c47qZlvyVDxt9BZtY8eO5fTTT+f0008nk8nQuXNnVq5cSUFBAUVFRZv1veiii5DE5ZdfDkDXrl158803adOmzVaPK2l+ROxf3bpGcUVTlaQ8SU9JelXSQkljk/bukpZI+m/gVaCrpPMlvSxpgaRLchu5mVn9Ki4u5rXXXmPIkCG8//77dO7cGYDOnTuzatWqLfrffffdHH/88QB89NFHAEyZMoVBgwbxve99j/fff3+bY2gsv0q0llSYLL8NfA84KiI+kdQeeEnSnGT9PsCEiPiJpEOBXsABgIA5kg6KiGcr71zSRGAiQPv2Hbi4f1n6Z9TIdWqd/S3LauYc1c45qpttyVMmk6lYXrduHWeeeSannnoqr776KmVlZZutr/p68eLFRASrV68mk8nw8ccfs2LFCnbddVeuueYa7rnnHn74wx9y0UUXbVP8jXLoTFJL4FfAQcBGssVlL+ArwJ8jYq+k39XAscBHyaZ5wJURcXNNx/LQWd14yKN2zlHtnKO6+SJDZ6WlpYwZM4ZRo0ZxzjnnALDPPvtsdejs7LPPpkOHDhWFJCLIy8vj008/pVmzZixfvpzDDjuMRYsWbXHcrQ2dNdZ/4ROBDsDgiCiVVEy2yACsqdRPZAvL7+q649Ytm1NUzRinbS6TyVB8YkGuw2jQnKPaOUd1s615ighOOeUU+vTpU1FkAL7zne8we/ZsJk2axOzZsxk7dmzFuo0bN3Lvvffy7LOfD/hI4sgjjySTyXDwwQfz1FNPse+++25z/I210OwKrEqKzAhgzxr6PQZcJumOiCiRtAdQGhFbDkyamTURzz//PLfddhv9+/eveHrsiiuuYNKkSRx33HHcfPPNdOvWjXvvvbdim2effZYuXbrQo0ePzfZ11VVX8cMf/pCzzjqLDh06cOutt25zPI210NwB/I+kV4BC4M3qOkXE45L6AC9mH0KjBBgHuNCYWZM1bNgwarot8tRTT1XbXlBQwEsvvbRF+5577rnZVc4X0SgKTdVHmyNiNTC0hu79qvS9FvBNFzOzHGmUjzebmVnj4UJjZmapcqExM7NUudCYmVmqXGjMzCxVLjRmZpYqFxozM0uVC42ZmaXKhcbMzFLlQmNmZqlyoTEzs1S50JiZWapcaMzM6ujkk0+mY8eO9Ov3+Wf3fv/73yc/P5/8/Hy6d+9e8bH8GzZsYMKECfTv35+BAwduNpPl5MmT6dq1K3l5eewIGl2hkXSUpJDUO9exmNmOZfz48Tz66KObtd19990UFhZSWFjIMcccw9FHHw3AjTfeCMDChQt54oknOPfcc9m4cSMARx55JPPmzavf4HOoUUwTUMXxwF+AHwBTt/fO15WW033S3O292ybn3P5ljHeetso5ql1jylHxtNEcdNBBFBcXV7s+Irjnnnt4+umnAVi8eDGHHHIIAB07dqRt27a88sorHHDAARx44IH1FXaD0KiuaCTlAf8HOIVsoUFSM0n/LWmRpIclPSLp2GTdYEnPSJov6TFJnXMYvpk1Yc899xydOnWiV69eAAwcOJCHHnqIsrIy3n77bebPn8/y5ctzHGVuNLYrmu8Cj0bEUkn/kjQI6AF0B/oDHYElwC2SWgK/AcZGxAeSvg9cDpxcdaeSJgITAdq378DF/cvq41watU6ts7+NWs2co9o1phxtusfy3nvvsWbNms3uuQD86le/4oADDqho33vvvXniiSfo3bs3nTp1onfv3ixZsmSz7crLy7fYT3VKSkrq1K+hUk3TfTZEkuYCMyPiCUlnAF2BlsDrEXFr0ud+4I9kp3d+AXgr2bw5sDIiDt3aMbr16BnNjvOEnLU5t38ZMxY2tt9T6pdzVLvGlKPiaaOz34uLGTNmDG+88UbFurKyMvbYYw/mz59Ply5dqt3+m9/8JjfddBP77rtvRVteXh4lJSW1HjuTyVBQUPDlTiBlkuZHxP7VrWsc/8KApN2Bg4F+koJs4QjggZo2ARZFRE1TPlerdcvmFCU/UFazTCZD8YkFuQ6jQXOOatdUcvTkk0/Su3fvzYrM2rVriQjatGnDE088QYsWLTYrMjuSxnSP5ljgDxGxZ0R0j4iuwNvAauCY5F5NJ6Ag6V8EdJA0FEBSS0l9cxG4mTUNxx9/PEOHDqWoqIguXbpw8803A3DXXXdx/PHHb9Z31apVDBo0iD59+nDVVVdx2223Vay74IIL6NKlC2vXrqVLly5MnTq1Pk+j3jWaKxqyT5tNq9J2H9AHWAG8ASwF/gp8HBEbkocCfi1pV7LnOhNYVG8Rm1mTcuedd1bb/vvf/36Ltu7du1NUVFRt/+nTpzN9+vTtGVqD1mgKTUQUVNP2a8g+jRYRJcnw2jxgYbK+EDioHsM0M7MqGk2hqcXDktoCOwGXRcR7OY7HzMwSTaLQVHe1Y2ZmDUNjehjAzMwaIRcaMzNLlQuNmZmlyoXGzMxS5UJjZmapcqExM7NUudCYmVmqXGjMzCxVLjRmZpYqFxqrVXl5Ofvttx9jxozZrP3qq69GEqtXrwagtLSUk046if79+9OnTx+uvPLKXIRrZg1MTguNpHJJhZLekHSvpK9upe9USefVZ3yWde2119KnT5/N2latWsUTTzxBt27dKtruvfdePvvsMxYuXMj8+fP53e9+V+P86ma248j1Z52ti4h8AEl3AKcB1+Q0oNJyuk+am8sQGoRNswmuWLGCuXPnMnnyZK655vN/mlmzZvHrX/+asWPHVrRJYs2aNZSVlbFu3Tp22mkndtlll3qP3cwaloY0dPYc0BNA0o8kLZD0uqTbqnaU9GNJLyfr79t0JSTpe8nV0euSnk3a+kqal1w5LZDUq17PqpE766yzmD59Os2aff6jMmfOHNq3b8/AgQM363vsscfSpk0bOnfuTLdu3TjvvPNo165dfYdsZg1Mgyg0kloAhwMLk1kwJwMHR8RA4MxqNrk/Ir6RrF8CnJK0XwyMStq/k7SdBlybXDntT3aSNKuDhx9+mI4dOzJ48OCKtrVr13L55ZczYcKELfrPmzeP5s2b8+677/L2228zY8YM3nrrrfoM2cwaoFwPnbWWVJgsPwfcDPwn8KeIWA0QEf+qZrt+kn4BtAXygMeS9ueB30u6B7g/aXsRmCypC9kCtazqziRNBCYCtG/fgYv7l22HU2vcMpkMd955J48//jj3338/GzZsYO3atRx22GEsXbqUU045BUl88MEH9O3bl+uuu44//OEP7Lvvvjz//PMA9OjRg9mzZzNixIgcn01ulJSUkMlkch1Gg+Yc1U1jz1OuC03FPZpNJAmIWrb7PfDdiHhd0nigACAiTpM0BBgNFErKj4g/Svpr0vaYpFMj4unKO4uIG4AbALr16BkzFuY6LblXfGIBBQUFFa8zmQxXX301Dz/8cMXrgoICunfvziuvvEL79u1ZtmwZb775JsOHD2ft2rX84x//4KqrrmLAgAE5Oovc2pQjq5lzVDeNPU8N8R31KeABSb+KiA8ltavmqmZnYKWklsCJwDsAkvaOiL8Cf5V0JNBV0q7AWxHxa0k9gAHA09SgdcvmFCU3wm3b/PSnP2XChAn069ePiGDChAk7bJExs881uEITEYskXQ48I6kceA0YX6XbFOCvwD+AhWQLD8Avk5v9IluwXgcmAeMklQLvAZemfhJNUEFBQbW/UVV+fDkvL4977723/oIys0Yhp4UmIvJqaJ8NzK7SNrXS8nXAddVsd3Q1u7sy+TIzsxxoEE+dmZlZ0+VCY2ZmqXKhMTOzVLnQmJlZqlxozMwsVS40ZmaWKhcaMzNLlQuNmZmlyoXGzMxS5UJjZmapcqExM7NUudCYmVmqXGisRuXl5ey3336MGTMGgClTpjBgwADy8/M5//zzeffddyv6LliwgKFDh9K3b1/69+/P+vXrcxW2mTUwDarQSJosaZGkBZIKJQ2RdJOkfZP1JTVsd6CkvybbLJE0tV4Db6KuvfZa+vTpU/H6/PPPZ8GCBRQWFnLggQdy6aXZGRfKysoYN24c119/PYsWLSKTydCyZctchW1mDUyDmY9G0lBgDDAoIj6T1B7YKSJOrcPms4Hjkhk3mwP7fNE41pWW033S3C+6eaNXnEz6tmLFCubOncvkyZO55pprANhll10q+q1fv54WLbI/Po8//jgDBgxg4MCBAOy+++71HLWZNWQN6YqmM7A6Ij4DiIjVEfGupIyk/Td1kjRD0quSnpLUIWnuCKxMtiuPiMVJ36mSbpP0tKRlkn5cz+fUaJ111llMnz6dZs02/xGZPHkyXbt25cknn6y4olm6dCmSGDVqFIMGDWL69Om5CNnMGqgGc0UDPA5cLGkp8CRwd0Q8U6VPG+DViDhX0sXAz4HTgV8BRZIywKPA7IjYdJNgAHBgsu1rkuZGxLuVdyppIjARoH37DlzcvyyVE2wMMpkML774IqWlpXz66acUFhby4YcfkslkABg5ciQjR47k1ltv5bzzzmPChAkUFRXx5JNPcv3119OqVSvOPfdcmjdvzuDBg3N7MjlWUlJSkTernnNUN409Tw2m0EREiaTBwLeAEcDdkiZV6bYRuDtZvh24P9n2Ukl3AIcCJwDHAwVJv4ciYh2wTtKfgQOAB6sc+wbgBoBuPXrGjIUNJi31rvjEAh577DHmz5/P+PHjWb9+PZ988gk33XQTt99+e0W/9957j1/84hfMnj2b9957j3Xr1jF27FgAXn75ZTZu3Fjt1M87kkwms8PnoDbOUd009jw1pKGzTcNemYjYdKVyTG2bVNr278kUz4cAAyXtXrVPDa+tiiuvvJIVK1ZQXFzMXXfdxcEHH8ztt9/OsmXLKvq88MIL9O7dG4BRo0axYMEC1q5dS1lZGc888wz77rtvrsI3swamTr+6S9obWJHcpC8gOxz1h4j4aHsFImkfYGNEbHo3ywf+AfSr1K0ZcCxwF9krl78k244GHomIAHoB5cCm2MZKupLs0FkBUPUqaTOtWzanKLkhbpubNGkSRUVFNGvWjLy8PO69914AdtttN8455xy+8Y1vIIkjjjiC0aOdQzPLqusY0X3A/pJ6AjcDc4A/Akdsx1jygN9IaguUAX8je9/kT5X6rAH6SpoPfAx8P2n/IfArSWuTbU+MiHJJAPOAuUA34LKq92ds6woKCiou2e+7776K9kwmwx577FHxety4cYwbN66+wzOzRqCuhWZjRJRJOgqYGRG/kfTa9gwkIuYD36xmVUGlPnnJ4pQq2/5gK7teGhETv3SAZmb2hdT1Hk2ppOOBk4CHkzb/RZ6ZmdWqrlc0E4DTgMsj4m1Je5F96qtBi4ipuY7BzGxHV6dCExGLJV1I9j4HEfE2MC3NwMzMrGmo09CZpCOBQrJ/DImkfElzUozLzMyaiLreo5lK9g8dPwKIiEJgr1QiMjOzJqWuhaYsIj6u0uY/fDQzs1rV9WGANySdADSX1As4A3ghvbDMzKypqOsVzc+AvsBnZP9Q82PgrJRiMjOzJqTWK5pkfpc5EfFtYHL6IZmZWVNS6xVNRJQDayXtWg/xmJlZE1PXezTrgYWSniD7eWMARMQZqURlZmZNRl0Lzdzky8zMbJvU6WGAiJhd3VfawVl61q9fzwEHHMDAgQPp27cvP//5zwEoLCzkwAMPJD8/n/3335958+Zttt0///lP8vLyuPvuu6vbrZnZFuo6H83bVPN3MxHRY7tHtB0kc+acFxFjchxKg9WqVSuefvpp8vLyKC0tZdiwYRx++OFcfPHF/PznP+fwww/nkUce4YILLthsCtmzzz6bww8/PHeBm1mjU9ehs/0rLX8F+B7QbvuHk3vrSsvpPqlpjxIWTxuNJPLysrMulJaWUlpaiiQk8cknnwDw8ccf87Wvfa1iuwcffJAePXrQpk0b3n///ZzEbmaNT12Hzj6s9PVORMwEDk4zMEndJb0p6SZJb0i6Q9K3JT0vaZmkA5KvFyS9lnzfp5r9tJF0i6SXk35j04y7MSkvLyc/P5+OHTsycuRIhgwZwsyZMzn//PPp2rUr5513HldeeSUAa9as4aqrrqoYYjMzq6u6Dp0NqvSyGdkrnJ1TiWhzPclePU0EXiY7ffMw4DvARcCPgIOSSdm+DVwBHFNlH5OBpyPi5GT2znmSnoyIiqfnJE1MjkH79h24uH9ZumeVY5WHwmbOnElJSQlTpkyhd+/e/M///A+nnHIKw4cP589//jNHH300M2bM4LrrruPQQw/llVdeobi4mGbNmm22H9tSSUmJc1QL56huGnueFFH7R5ZJ+nOll2XA28CMiChKLTCpO/BERPRKXv8BeCwi7pDUA7gfOBL4NdCL7D2klhHRu/I9GkmvkB3u21Q92gGjImJJdcft1qNnNDvu2rROq0EonjZ6i7ZLLrmENm3acNlll/HRRx8hiYhg11135ZNPPuFb3/oWy5cvB+Cjjz5i48aNXHHFFZx++un1HX6jkclkKqbBtuo5R3XTGPIkaX5E7F/durreozklIt6qstP6+PTmzyotb6z0eiPZ2C8D/hwRRyWFKVPNPgQcU9ei2Lplc4qqeSNuaj744ANatmxJ27ZtWbduHU8++SQXXnghX/va13jmmWcoKCjg6aefplevXgA899xzFdtOnTqV999/30XGzOqkroXmT8CgatoGb99wttmuwDvJ8vga+jwG/EzSzyIiJO0XEa/VS3QN2MqVKznppJMoLy9n48aNHHfccYwZM4a2bdty5plnUlZWxle+8hVuuOGGXIdqZo3cVguNpN5kP0xzV0lHV1q1C9nhqFybDsyWdA7wdA19LgNmAgskCSgGdvjHngcMGMBrr21Zb4cNG8b8+fO3uu3UqVMb9XixmdWv2q5o9iH7ptyW7P2QTT4FfpxSTABERDHQr9Lr8TWs+3qlzaYk6zMkw2gRsQ74zxRDNTOzrdhqoYmIh4CHJA2NiBfrKSYzM2tC6nqP5jVJPyU7jFYxZBYRJ6cSlZmZNRl1nfjsNuA/gFHAM0AXssNnZmZmW1XXQtMzIqYAa5IP0xwN9E8vLDMzayrqWmhKk+8fSepH9rHi7qlEZGZmTUpd79HcIGk3sk91zQHygItTi8rMzJqMOhWaiLgpWXwGaJBTA5iZWcNUp6EzSZ0k3Szpf5PX+0o6Jd3QzMysKajrPZrfk/0ol02TkywFzkohHjMza2LqWmjaR8Q9ZD/MkogoA8pTi8rMzJqMuhaaNZJ2J5nOWdKBwMepRWVmZk1GXZ86O4fs02Z7S3oe6AAcm1pUZmbWZGz1ikZSN4CIeBUYDnyT7AdU9o2IBemHZ9vT+vXrOeCAAxg4cCB9+/bdbFrm3/zmN+yzzz707duXCy64AIDi4mJat25Nfn4++fn5nHbaabkK3cwasdquaB7k83lo7o6IqtMkb1eSyoGFSVxLgJMiYu2X2F934OGI6Fdb3x1Bq1atePrpp8nLy6O0tJRhw4Zx+OGHs27dOh566CEWLFhAq1atWLVqVcU2e++9N4WFhbkL2swavdoKjSot18ffz6yLiHwASXcApwHX1LaRpBbJAwpfPoDScrpPmrs9dtWgFE8bjSTy8vIAKC0tpbS0FElcd911TJo0iVatWgHQsWPHXIZqZk1MbQ8DRA3L9eE5oKekIyX9VdJrkp6U1AlA0lRJN0h6HPhD8rc+D0h6Pfn6ZrKf5pJulLRI0uOSWtfzeTQo5eXl5Ofn07FjR0aOHMmQIUNYunQpzz33HEOGDGH48OG8/PLLFf3ffvtt9ttvP4YPH77ZdM5mZnWliJrrRzKUtYbslU1rYNMwloCIiF22azBSSUTkSWoB3Ac8CtwFfJRMw3wq0CcizpU0lexkbMMiYp2ku4EXI2KmpOZkPyZnN+BvwP4RUSjpHmBORNxe5bgTgYkA7dt3GHzxzBu352k1CP332HWz1yUlJUyZMoUzzjiDSy+9lP3224+f/exnvPnmm1x66aX88Y9/pLS0lHXr1rHrrrtSVFTElClTuPXWW2nTpg0lJSUVV0dWPeeods5R3TSGPI0YMWJ+ROxf3braJj5rnk5INWotqTBZfg64mewsn3dL6gzsBLxdqf+cZAZNgIOBHwFERDnwcfL5bG9HxKZ9zqeaDwONiBuAGwC69egZMxbW9WG8xqP4xIIt2ubPn8+HH37IPvvswxlnnEFBQQEjRozg6quvpl+/fnTo0KGib0FBAXfeeSedOnVi//33J5PJUFCw5T7tc85R7ZyjumnseWpo76gV92g2kfQb4JqImCOpAJhaafWaOuzzs0rL5WSvzGrUumVziqaNrkusjc4HH3xAy5Ytadu2LevWrePJJ5/kwgsvJC8vj6effpqCggKWLl3Khg0baN++PR988AHt2rWjefPmvPXWWyxbtowePfxRd2a2bRpaoanOrsA7yfJJW+n3FPB/gU1DZ23SDqyxWblyJSeddBLl5eVs3LiR4447jjFjxrBhwwZOPvlk+vXrx0477cTs2bORxLPPPsvFF19MixYtaN68Oddffz3t2rXL9WmYWSPTGArNVOBeSe8ALwF71dDvTLLTGZxC9srl/wIr6yXCRmLAgAG89tprW7TvtNNO3H777Vu0H3PMMRxzTKpPtJvZDqBBFZqI2OJuV0Q8BDxUTfvUKq/fB8ZWs9t+lfpc/eWjNDOzbVHXzzozMzP7QlxozMwsVS40ZmaWKhcaMzNLlQuNmZmlyoXGzMxS5UJjZmapcqExM7NUudCYmVmqXGjMzCxVLjRmZpYqF5pGbvny5YwYMYI+ffrQt29frr32WgBef/11hg4dSv/+/TnyyCP55JNPKra58sor6dmzJ/vssw+PPfZYrkI3sx3EDlFoJE1OpnJeIKlQ0pBcx7S9tGjRghkzZrBkyRJeeuklZs2axeLFizn11FOZNm0aCxcu5KijjuKXv/wlAIsXL+auu+5i0aJFPProo/zkJz+hvLw8x2dhZk1Zg/r05jRIGgqMAQZFxGeS2pOdqbNa60rL6T5pbr3F92UUTxtN586d6dy5MwA777wzffr04Z133qGoqIiDDjoIgJEjRzJq1Cguu+wyHnroIX7wgx/QqlUr9tprL3r27Mm8efMYOnRoLk/FzJqwHeGKpjOwOiI+A4iI1RHxbo5jSkVxcTGvvfYaQ4YMoV+/fsyZMweAe++9l+XLlwPwzjvv0LVr14ptunTpwjvvvFPt/szMtocdodA8DnSVtFTSf0sanuuA0lBSUsIxxxzDzJkz2WWXXbjllluYNWsWgwcP5tNPP2WnnbIXcRGxxbaS6jtcM9uBNPmhs4gokTQY+BYwArhb0qSI+P2mPpImAhMB2rfvwMX9y3IS67bKZDIAlJWV8V//9V8MGTKEdu3aVbRfdNFFQPaBgY4dO5LJZNiwYQPPPPMMXbp0AWDBggUMGjSoYpu6Kikp2eZtdjTOUe2co7pp7HlSdb/hNmWSjgVOiogjq1vfrUfPaHbctfUc1RdTPG00EcFJJ51Eu3btmDlzZsW6VatW0bFjRzZu3Mj48eMpKCjg5JNPZtGiRZxwwgnMmzePd999l0MOOYRly5bRvHnzbTp2JpOhoKBg+55QE+Mc1c45qpvGkCdJ8yNi/+rWNfkrGkn7ABsjYlnSlA/8o6b+rVs2p2ja6PoIbbt4/vnnue222+jfvz/5+fkAXHHFFSxbtoxZs2YBcPTRRzNhwgQA+vbty3HHHce+++5LixYtmDVr1jYXGTOzbdHkCw2QB/xGUlugDPgbyTBZUzBs2LBq77sAnHnmmdW2T548mcmTJ6cZlplZhSZfaCJiPvDNXMdhZraj2hGeOjMzsxxyoTEzs1S50JiZWapcaMzMLFUuNGZmlioXGjMzS5ULjZmZpcqFxszMUuVCY2ZmqXKhMTOzVLnQmJlZqlxozMwsVS40OXDyySfTsWNH+vXrV9E2depU9thjD/Lz88nPz+eRRx6pWLdgwQKGDh1K37596d+/P+vXr89F2GZmX0iTLjSSukh6SNIySW9J+q2kVrmOa/z48Tz66KNbtJ999tkUFhZSWFjIEUccAWRnzxw3bhzXX389ixYtIpPJ0LJly/oO2czsC2uy0wRIEnA/cF1EjJXUHLgBmA5UP1ELsK60nO6T5qYSU3EyodpBBx1EcXFxnbZ5/PHHGTBgAAMHDgRg9913TyU2M7O0NOUrmoOB9RFxK0BElANnAz+SlJfTyGrw29/+lgEDBnDyySfz73//G4ClS5ciiVGjRjFo0CCmT5+e4yjNzLZNk72iAfoC8ys3RMQnkoqBnkDhpnZJE0lm3WzfvgMX9y9LJaBMJlOx/N5777FmzZqKtgEDBnDzzTcjiVtuuYUTTjiBCy+8kKKiIp588kmuv/56WrVqxbnnnkvz5s0ZPHhwKjHWVUlJyWbnY1tyjmrnHNVNY89TUy40Aqqb41hVGyLiBrLDanTr0TNmLEwnLcUnFny+XFxMmzZtKCgo2KJfjx49GDNmDAUFBbz33nusW7eOsWPHAvDyyy+zcePGarerT5lMJucxNHTOUe2co7pp7HlqyoVmEXBM5QZJuwCdgKKaNmrdsjlFyb2U+rRy5Uo6d+4MwAMPPFDxRNqoUaOYPn06a9euZaedduKZZ57h7LPPrvf4zMy+qKZcaJ4Cpkn6UUT8IXkYYAbw24hYl8vAjj/+eDKZDKtXr6ZLly5ccsklZDIZCgsLkUT37t353e9+B8Buu+3GOeecwze+8Q0kccQRRzB6dP0XQjOzL6rJFpqICElHAbMkTQE6AHdHxOU5Do0777xzi7ZTTjmlxv7jxo1j3LhxaYZkZpaapvzUGRGxPCK+ExG9gCOAwyTl9i66mdkOpsle0VQVES8Ae+Y6DjOzHU2TvqIxM7Pcc6ExM7NUudCYmVmqXGjMzCxVLjRmZpYqFxozM0uVC42ZmaXKhcbMzFLlQmNmZqlyoTEzs1S50JiZWapcaLaza6+9ln79+tG3b19mzpwJwJQpUxgwYAD5+fkceuihvPvuu7kN0sysHjW5QiPphVwd+4033uDGG29k3rx5vP766zz88MMsW7aM888/nwULFlBYWMiYMWO49NJLcxWimVm9a3Kf3hwR3/wy268rLaf7pLnbvF3xtNEsWbKEAw88kK9+9asADB8+nAceeIALLrigot+aNWuQtphN2sysyUrlikbSZZLOrPT6cklnSvqlpDckLZT0/WRdgaSHK/X9raTxyXKxpEskvZps0ztp7yDpiaT9d5L+Ial9sq6k0n4zkv4k6U1Jdyjld/h+/frx7LPP8uGHH7J27VoeeeQRli9fDsDkyZPp2rUrd9xxh69ozGyHoojY/juVugP3R8QgSc2AZcAFwGnAYUB74GVgCLAPcF5EjEm2/S3wSkT8XlIxMCMifiPpJ8CgiDg16fNORFwp6TDgf4EOEbFaUklE5EkqAB4C+gLvAs8D50fEX6qJdyIwEaB9+w6DL5554zafc/89dgVg7ty5PPTQQ7Ru3Zo999yTVq1a8dOf/rSi3x133MGGDRuYMGHCNh+jISkpKSEvLy/XYTRozlHtnKO6aQx5GjFixPyI2L+6dakMnUVEsaQPJe0HdAJeA4YBd0ZEOfC+pGeAbwCf1LK7+5Pv84Gjk+VhwFHJsR6V9O8atp0XESsAJBUC3YEtCk1E3ADcANCtR8+YsXDb01J8YgEABQUF/PKXvwTgoosuokuXLhQUFFT022uvvRg9ejSzZ8/e5mM0JJlMZrPzsi05R7VzjuqmsecpzXs0NwHjgf8AbgEOraFfGZsP4X2lyvrPku/lfB5vXYfAPqu0XHn7GrVu2ZyiaaPruPstrVq1io4dO/LPf/6T+++/nxdffJFly5bRq1cvAObMmUPv3r2/8P7NzBqbNAvNA8ClQEvgBLIF5D8lzQbaAQcB5yfr95XUKulzCNVcdVTxF+A44CpJhwK7pXIGX8AxxxzDhx9+SMuWLZk1axa77bYbp556KkVFRTRr1ow999yT66+/PtdhmpnVm9QKTURskPRn4KOIKJf0ADAUeB0I4IKIeA9A0j3AArL3cl6rw+4vAe5MHih4BlgJfJrCaWyz5557bou2++67LweRmJk1DKkVmuQhgAOB7wFE9qmD85OvzUTEBWQfFqja3r3S8itAQfLyY2BURJRJGgqMiIjPkn55yfcMkKm0/elf/qzMzGxbpVJoJO0LPAw8EBHLUjhEN+CepJhtAH6cwjHMzGw7SOups8VAjzT2nex/GbBfWvs3M7Ptp8l9BI2ZmTUsLjRmZpYqFxozM0uVC42ZmaXKhcbMzFLlQmNmZqlyoTEzs1S50JiZWapcaMzMLFUuNGZmlioXGjMzS5ULjZmZpcqFxszMUuVCY2ZmqVJ2PjLbRNKnQFGu42gE2gOrcx1EA+cc1c45qpvGkKc9I6JDdStSm2GzESuKiP1zHURDJ+kV52nrnKPaOUd109jz5KEzMzNLlQuNmZmlyoVmSzfkOoBGwnmqnXNUO+eobhp1nvwwgJmZpcpXNGZmlioXGjMzS5ULTSWSDpNUJOlvkiblOp5cklQsaaGkQkmvJG3tJD0haVnyfbdK/f8ryVuRpFG5izw9km6RtErSG5XatjknkgYnuf2bpF9LUn2fS5pqyNNUSe8kP0+Fko6otG6Hy5OkrpL+LGmJpEWSzkzam+bPU0T4K3ufqjnwd6AHsBPwOrBvruPKYT6KgfZV2qYDk5LlScBVyfK+Sb5aAXsleWye63NIIScHAYOAN75MToB5wFBAwP8Ch+f63OohT1OB86rpu0PmCegMDEqWdwaWJrlokj9PvqL53AHA3yLirYjYANwFjM1xTA3NWGB2sjwb+G6l9rsi4rOIeBv4G9l8NikR8SzwryrN25QTSZ2BXSLixci+S/yh0jZNQg15qskOmaeIWBkRrybLnwJLgD1ooj9PLjSf2wNYXun1iqRtRxXA45LmS5qYtHWKiJWQ/Y8CdEzad+TcbWtO9kiWq7bvCE6XtCAZWts0JLTD50lSd2A/4K800Z8nF5rPVTeuuSM/+/1/ImIQcDjwU0kHbaWvc7elmnKyo+bqOmBvIB9YCcxI2nfoPEnKA+4DzoqIT7bWtZq2RpMnF5rPrQC6VnrdBXg3R7HkXES8m3xfBTxAdijs/eRSneT7qqT7jpy7bc3JimS5anuTFhHvR0R5RGwEbuTzodUdNk+SWpItMndExP1Jc5P8eXKh+dzLQC9Je0naCfgBMCfHMeWEpDaSdt60DBwKvEE2Hycl3U4CHkqW5wA/kNRK0l5AL7I3KHcE25STZDjkU0kHJk8H/ajSNk3WpjfPxFFkf55gB81Tck43A0si4ppKq5rmz1Oun0ZoSF/AEWSf/vg7MDnX8eQwDz3IPuHyOrBoUy6A3YGngGXJ93aVtpmc5K2IBvjUy3bKy51kh31Kyf4mecoXyQmwP9k32r8DvyX5hI6m8lVDnm4DFgILyL5pdt6R8wQMIzvEtQAoTL6OaKo/T/4IGjMzS5WHzszMLFUuNGZmlioXGjMzS5ULjZmZpcqFxszMUtUi1wGY7SgklZN9xHeT70ZEcY7CMas3frzZrJ5IKomIvHo8XouIKKuv45nVxENnZg2EpM6Snk3ma3lD0reS9sMkvSrpdUlPJW3tJD2YfEjlS5IGJO1TJd0g6XHgD5I6SLpP0svJ1//J4SnaDspDZ2b1p7WkwmT57Yg4qsr6E4DHIuJySc2Br0rqQPazwQ6KiLcltUv6XgK8FhHflXQw2Y+Hz0/WDQaGRcQ6SX8EfhURf5HUDXgM6JPaGZpVw4XGrP6si4j8rax/Gbgl+bDFByOiUFIB8Gxk5yAhIjbN8zIMOCZpe1rS7pJ2TdbNiYh1yfK3gX0rTbq4i6SdIzsHilm9cKExayAi4tlkOobRwG2Sfgl8RPUf+761j4dfU6mtGTC0UuExq3e+R2PWQEjaE1gVETeS/WTfQcCLwPDkE3upNHT2LHBi0lYArI7q5zN5HDi90jHyUwrfrEa+ojFrOAqA8yWVAiXAjyLig2SG0/slNSM7P8lIYCpwq6QFwFo+/2j5qs4AZiX9WpAtUKelehZmVfjxZjMzS5WHzszMLFUuNGZmlioXGjMzS5ULjZmZpcqFxszMUuVCY2ZmqXKhMTOzVP1/OVLMsPgLrqcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8754\n",
      "AUC Score (Train): 0.924280\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807175\n",
      "F1: 0.703448\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 675 candidates, totalling 3375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1204 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 3204 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3375 out of 3375 | elapsed:  1.3min finished\n",
      "C:\\Users\\Yung\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:849: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs...\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs...\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.35440087, 0.08600135, 0.11040001, 0.02600088, 0.07240105,\n",
       "        0.10920191, 0.02439942, 0.06860075, 0.110601  , 0.03120012,\n",
       "        0.07440124, 0.11780133, 0.02600045, 0.0754003 , 0.1164022 ,\n",
       "        0.02559958, 0.07280006, 0.1246017 , 0.03060074, 0.09240251,\n",
       "        0.14280066, 0.02860069, 0.08760128, 0.13920102, 0.02919998,\n",
       "        0.08019934, 0.14040198, 0.03700013, 0.09520054, 0.15740023,\n",
       "        0.03460064, 0.09160085, 0.14420223, 0.03220048, 0.08920078,\n",
       "        0.1389998 , 0.03600039, 0.09740148, 0.16100111, 0.03479943,\n",
       "        0.09640088, 0.15160165, 0.03300009, 0.08820009, 0.14520102,\n",
       "        0.02359843, 0.06080046, 0.10460191, 0.02320085, 0.06140046,\n",
       "        0.1014008 , 0.02320013, 0.06280022, 0.0942019 , 0.02539949,\n",
       "        0.07020049, 0.11480179, 0.02519975, 0.07460198, 0.11520424,\n",
       "        0.02880087, 0.07360091, 0.11240191, 0.03020015, 0.0808002 ,\n",
       "        0.13380218, 0.02979913, 0.08140178, 0.12940183, 0.02859983,\n",
       "        0.08020024, 0.12780218, 0.03260121, 0.09320073, 0.15040088,\n",
       "        0.03240113, 0.0886013 , 0.14420257, 0.03160028, 0.08760123,\n",
       "        0.16080132, 0.03660231, 0.10500145, 0.18240104, 0.03620014,\n",
       "        0.1256011 , 0.16400094, 0.03460059, 0.09820113, 0.15420055,\n",
       "        0.02479916, 0.06380134, 0.10800023, 0.02520027, 0.06260118,\n",
       "        0.1030004 , 0.023     , 0.0624002 , 0.10160208, 0.02800102,\n",
       "        0.07080131, 0.11640034, 0.02720137, 0.07240086, 0.12300158,\n",
       "        0.02780037, 0.07760139, 0.11760025, 0.03120198, 0.08600106,\n",
       "        0.13820133, 0.03500061, 0.08000045, 0.14240007, 0.03219953,\n",
       "        0.08160062, 0.13600121, 0.03640027, 0.09980202, 0.15600171,\n",
       "        0.03520017, 0.09020104, 0.15340152, 0.03320031, 0.08980155,\n",
       "        0.14420166, 0.03760085, 0.10300183, 0.17060027, 0.03340082,\n",
       "        0.09180145, 0.17240067, 0.0365993 , 0.10180097, 0.17240171,\n",
       "        0.02420039, 0.07320023, 0.11840153, 0.03139982, 0.06940098,\n",
       "        0.11279988, 0.02419991, 0.07080154, 0.10800014, 0.03539934,\n",
       "        0.0820013 , 0.11940079, 0.02939978, 0.08280096, 0.12680249,\n",
       "        0.02659893, 0.07540064, 0.12340021, 0.03079987, 0.08960123,\n",
       "        0.14100094, 0.03779907, 0.0842011 , 0.13340096, 0.02919989,\n",
       "        0.07880158, 0.13460054, 0.03240108, 0.09220386, 0.14920011,\n",
       "        0.03480053, 0.09160128, 0.14659901, 0.03259969, 0.08840189,\n",
       "        0.13960009, 0.03880029, 0.10780096, 0.17080083, 0.03440084,\n",
       "        0.10040088, 0.15660086, 0.03439989, 0.09320073, 0.1526001 ,\n",
       "        0.02259903, 0.06340032, 0.10200148, 0.02439933, 0.06199985,\n",
       "        0.10499988, 0.02460003, 0.06440053, 0.10160055, 0.0272007 ,\n",
       "        0.07219992, 0.119801  , 0.02859983, 0.07100081, 0.11800094,\n",
       "        0.026401  , 0.07480083, 0.11960187, 0.03080015, 0.08860202,\n",
       "        0.13579969, 0.03080034, 0.08400192, 0.13460159, 0.03019962,\n",
       "        0.08740201, 0.13840237, 0.03559952, 0.09560061, 0.15680742,\n",
       "        0.03700104, 0.09400759, 0.1560081 , 0.03260055, 0.08960724,\n",
       "        0.14700074, 0.0370008 , 0.10200124, 0.17180133, 0.03359995,\n",
       "        0.09840136, 0.16540084, 0.0340004 , 0.09960151, 0.15900288,\n",
       "        0.023     , 0.06240044, 0.09840279, 0.02379909, 0.0579999 ,\n",
       "        0.10000114, 0.02299991, 0.06299915, 0.09400039, 0.02880149,\n",
       "        0.06600175, 0.11460104, 0.02659998, 0.07020078, 0.11460142,\n",
       "        0.02719913, 0.07100029, 0.12000084, 0.0317996 , 0.088201  ,\n",
       "        0.14320102, 0.03060026, 0.08380017, 0.13880243, 0.02920094,\n",
       "        0.08060079, 0.12620134, 0.03399935, 0.09000044, 0.1452004 ,\n",
       "        0.03420048, 0.09060211, 0.14320202, 0.03100085, 0.08940067,\n",
       "        0.14040146, 0.03520098, 0.10500107, 0.16040058, 0.03479986,\n",
       "        0.09260125, 0.15640078, 0.03440013, 0.09080067, 0.15100131,\n",
       "        0.02279925, 0.05920053, 0.09800129, 0.02860017, 0.0599998 ,\n",
       "        0.09960213, 0.02299995, 0.06520081, 0.10980129, 0.02739992,\n",
       "        0.07520041, 0.13560085, 0.02900023, 0.0764008 , 0.1340013 ,\n",
       "        0.02959971, 0.08580055, 0.12560129, 0.03819976, 0.09500017,\n",
       "        0.14499993, 0.03419852, 0.08940072, 0.14540095, 0.03360066,\n",
       "        0.09080067, 0.14000187, 0.04200172, 0.09960146, 0.14920058,\n",
       "        0.03879995, 0.09220181, 0.14800043, 0.03320003, 0.09360094,\n",
       "        0.14580088, 0.03739972, 0.10380206, 0.16820011, 0.0359993 ,\n",
       "        0.09740062, 0.15880036, 0.03379979, 0.09260182, 0.15400085,\n",
       "        0.02460084, 0.06440072, 0.10059853, 0.02640057, 0.06480031,\n",
       "        0.09660082, 0.02320027, 0.06420078, 0.09800229, 0.02699986,\n",
       "        0.07279997, 0.11720872, 0.02780023, 0.07280641, 0.11480718,\n",
       "        0.02740631, 0.06920152, 0.11780095, 0.03059974, 0.08060079,\n",
       "        0.13680096, 0.03480182, 0.0808012 , 0.13900366, 0.02980013,\n",
       "        0.08800106, 0.13200274, 0.03400102, 0.09380245, 0.14800034,\n",
       "        0.03739963, 0.08840241, 0.14320273, 0.03539891, 0.08680038,\n",
       "        0.14440169, 0.03560162, 0.09920111, 0.16400194, 0.03600044,\n",
       "        0.09640112, 0.16400065, 0.03479986, 0.0982008 , 0.15220037,\n",
       "        0.0236001 , 0.05980096, 0.10540042, 0.02259955, 0.06300077,\n",
       "        0.10100145, 0.02299967, 0.06260076, 0.10100145, 0.02740064,\n",
       "        0.07320185, 0.11880112, 0.02560077, 0.07479992, 0.11820097,\n",
       "        0.02700005, 0.07260022, 0.11500063, 0.03080025, 0.0828011 ,\n",
       "        0.13699999, 0.03179941, 0.08259997, 0.13320079, 0.02979999,\n",
       "        0.08180008, 0.13960066, 0.0327992 , 0.09560099, 0.15500031,\n",
       "        0.03260016, 0.09140306, 0.14500051, 0.03360038, 0.08880005,\n",
       "        0.1460011 , 0.03720007, 0.10280051, 0.17059975, 0.03779955,\n",
       "        0.09780097, 0.15600057, 0.03719921, 0.097401  , 0.15340023,\n",
       "        0.02319999, 0.06420026, 0.1085999 , 0.02279873, 0.05960007,\n",
       "        0.10220108, 0.0234005 , 0.06340013, 0.10240254, 0.02720013,\n",
       "        0.07560301, 0.12520251, 0.02719784, 0.07180071, 0.12060065,\n",
       "        0.02639918, 0.07240162, 0.11780124, 0.0314003 , 0.08580184,\n",
       "        0.145401  , 0.03520103, 0.09160113, 0.15340042, 0.02939978,\n",
       "        0.08460174, 0.154001  , 0.03780065, 0.10960116, 0.17260032,\n",
       "        0.03699999, 0.10440197, 0.16360078, 0.03560004, 0.0924005 ,\n",
       "        0.15620112, 0.03999987, 0.11840076, 0.16780066, 0.04539943,\n",
       "        0.10520144, 0.1634007 , 0.03459997, 0.0972002 , 0.15900126,\n",
       "        0.02260027, 0.06139975, 0.09719982, 0.02180014, 0.05980005,\n",
       "        0.09039993, 0.02140031, 0.0575995 , 0.09680095, 0.02439966,\n",
       "        0.06839957, 0.10600095, 0.0268003 , 0.06639981, 0.113801  ,\n",
       "        0.02480011, 0.06500015, 0.10300155, 0.02659845, 0.07000017,\n",
       "        0.12480168, 0.02840066, 0.07100024, 0.12320089, 0.02819977,\n",
       "        0.07280045, 0.11880255, 0.02880063, 0.08000088, 0.13360076,\n",
       "        0.02899971, 0.08000093, 0.12500052, 0.03040009, 0.07540021,\n",
       "        0.14160156, 0.03300076, 0.09160242, 0.15200076, 0.03780065,\n",
       "        0.08360095, 0.14300056, 0.02800031, 0.0788002 , 0.13360009,\n",
       "        0.02179942, 0.0616003 , 0.09420114, 0.02159967, 0.05779924,\n",
       "        0.09400048, 0.02279987, 0.06140032, 0.09680037, 0.02420025,\n",
       "        0.06860089, 0.1386003 , 0.02900014, 0.08960075, 0.13220081,\n",
       "        0.02939997, 0.0788003 , 0.11760206, 0.03300042, 0.08419929,\n",
       "        0.12720103, 0.03260036, 0.07440143, 0.12880058, 0.03119993,\n",
       "        0.08020039, 0.12780166, 0.03540053, 0.09020085, 0.15760107,\n",
       "        0.0361083 , 0.09120121, 0.1542181 , 0.03179998, 0.09130907,\n",
       "        0.14200282, 0.03280072, 0.09120202, 0.14060078, 0.03100104,\n",
       "        0.08240051, 0.13360248, 0.03119969, 0.0816011 , 0.13240113,\n",
       "        0.02300072, 0.06020031, 0.09220185, 0.02099943, 0.06040092,\n",
       "        0.09400001, 0.02720041, 0.06100006, 0.09260178, 0.02559967,\n",
       "        0.06600065, 0.1144022 , 0.02380052, 0.06600089, 0.10600257,\n",
       "        0.02360072, 0.06640062, 0.11000147, 0.02940078, 0.07560139,\n",
       "        0.13820701, 0.03040066, 0.08560147, 0.12220087, 0.03140683,\n",
       "        0.08320007, 0.11840057, 0.02900033, 0.08020144, 0.13880138,\n",
       "        0.02900038, 0.08120122, 0.12600112, 0.03040047, 0.07580113,\n",
       "        0.13060141, 0.03700023, 0.09440217, 0.15780044, 0.03439846,\n",
       "        0.09360147, 0.14900436, 0.0318007 , 0.08819938, 0.15980062,\n",
       "        0.02279968, 0.07620029, 0.11460013, 0.02920089, 0.07499938,\n",
       "        0.11560149, 0.02319989, 0.07400064, 0.10659976, 0.027     ,\n",
       "        0.06780138, 0.11440148, 0.02739968, 0.07400002, 0.12120271,\n",
       "        0.02499981, 0.07480168, 0.12260108, 0.02760062, 0.07980695,\n",
       "        0.12820234, 0.02680092, 0.0756011 , 0.12460184, 0.02540007,\n",
       "        0.07340059, 0.11739984, 0.03100023, 0.08220129, 0.13380098,\n",
       "        0.02880025, 0.0796001 , 0.12880197, 0.03219991, 0.08160119,\n",
       "        0.1252018 , 0.03059945, 0.08940201, 0.15300083, 0.03380051,\n",
       "        0.09399891, 0.15940104, 0.03259912, 0.0934    , 0.15040112,\n",
       "        0.02560048, 0.06440072, 0.10960097, 0.02359986, 0.06140194,\n",
       "        0.09960065, 0.02319984, 0.06120086, 0.09620199, 0.02619967,\n",
       "        0.0700006 , 0.11520123, 0.02479897, 0.06699944, 0.11180053,\n",
       "        0.02320027, 0.06760106, 0.11060119, 0.02640085, 0.07500219,\n",
       "        0.12660255, 0.02700152, 0.07380075, 0.12500052, 0.02619958,\n",
       "        0.07719979, 0.11740112, 0.03100052, 0.0816009 , 0.13280082,\n",
       "        0.03060017, 0.0812007 , 0.13400126, 0.03020029, 0.07640038,\n",
       "        0.13100104, 0.03039961, 0.09180212, 0.14459991, 0.03760004,\n",
       "        0.08559976, 0.14760008, 0.03459973, 0.08659997, 0.133006  ]),\n",
       " 'std_fit_time': array([1.63397693e-01, 2.32879379e-02, 4.17596611e-03, 4.04894033e-03,\n",
       "        1.32006046e-02, 5.07526003e-03, 4.31686183e-03, 3.87771533e-03,\n",
       "        6.62177569e-03, 3.12285090e-03, 6.08657546e-03, 1.40765325e-02,\n",
       "        2.52935607e-03, 6.88795048e-03, 5.74797334e-03, 8.00240865e-04,\n",
       "        4.35498284e-03, 6.53105704e-03, 4.75753373e-03, 1.20928175e-02,\n",
       "        8.97543063e-03, 2.24449833e-03, 8.18719681e-03, 8.74948680e-03,\n",
       "        9.79394313e-04, 4.62135753e-03, 6.83109481e-03, 4.60652906e-03,\n",
       "        8.93169740e-03, 1.30013127e-02, 2.41744829e-03, 3.38193218e-03,\n",
       "        8.30438805e-03, 7.48481182e-04, 8.37619629e-03, 5.62121782e-03,\n",
       "        1.16410786e-06, 3.38244835e-03, 4.56090437e-03, 9.79978305e-04,\n",
       "        3.92929810e-03, 4.13032362e-03, 8.94042849e-04, 2.63775178e-03,\n",
       "        5.84513444e-03, 1.85572638e-03, 4.79287849e-03, 6.24829352e-03,\n",
       "        7.47602119e-04, 2.87071073e-03, 6.15111328e-03, 2.48226214e-03,\n",
       "        5.80888145e-03, 3.37201819e-03, 4.89259170e-04, 2.13665886e-03,\n",
       "        4.87264569e-03, 1.16677684e-03, 7.70992399e-03, 8.37218126e-03,\n",
       "        6.40036614e-03, 4.88228205e-03, 2.57631127e-03, 9.79393977e-04,\n",
       "        2.31455476e-03, 5.19297086e-03, 1.72035226e-03, 5.53498844e-03,\n",
       "        3.87882195e-03, 2.24536296e-03, 5.19255567e-03, 4.39946474e-03,\n",
       "        1.01973011e-03, 7.67837997e-03, 7.05955223e-03, 2.41649223e-03,\n",
       "        6.62254769e-03, 4.79291865e-03, 1.35681079e-03, 1.74366842e-03,\n",
       "        2.68576984e-02, 4.89145832e-04, 3.28779514e-03, 2.05278120e-02,\n",
       "        2.92621056e-03, 3.26231489e-02, 7.37576689e-03, 1.35661404e-03,\n",
       "        7.44238009e-03, 2.48177034e-03, 2.31522629e-03, 2.56104567e-03,\n",
       "        7.45747115e-03, 3.76304963e-03, 3.13765344e-03, 3.74215084e-03,\n",
       "        1.67379727e-03, 4.88227244e-03, 6.58996370e-03, 4.19436315e-03,\n",
       "        7.02656300e-03, 6.18367521e-03, 2.92627026e-03, 5.60632207e-03,\n",
       "        7.29318077e-03, 3.05982908e-03, 6.53041296e-03, 4.75844345e-03,\n",
       "        1.46836261e-03, 8.39073792e-03, 5.77634582e-03, 6.57252080e-03,\n",
       "        3.63374397e-03, 9.39432401e-03, 6.49256004e-03, 5.16109322e-03,\n",
       "        6.35729671e-03, 3.38290201e-03, 8.74998491e-03, 1.10452211e-02,\n",
       "        1.72102227e-03, 6.24174865e-03, 6.62079926e-03, 1.71985825e-03,\n",
       "        3.05845528e-03, 6.76510481e-03, 2.72752616e-03, 5.89885473e-03,\n",
       "        6.94493009e-03, 1.49572039e-03, 1.72108889e-03, 1.39217768e-02,\n",
       "        2.41638285e-03, 8.42492013e-03, 1.44016503e-02, 1.32684176e-03,\n",
       "        9.74393219e-03, 1.83898144e-02, 6.88793528e-03, 7.39131688e-03,\n",
       "        4.06919486e-03, 1.60000329e-03, 3.30889459e-03, 8.41424195e-03,\n",
       "        1.03077024e-02, 7.77156707e-03, 3.61123904e-03, 1.20011172e-03,\n",
       "        5.87923508e-03, 6.07906530e-03, 2.15466716e-03, 5.71334468e-03,\n",
       "        1.06121030e-02, 2.48263486e-03, 5.38941436e-03, 9.79769657e-03,\n",
       "        5.26892444e-03, 3.65536234e-03, 9.26428448e-03, 7.48889090e-04,\n",
       "        2.63861551e-03, 6.82912159e-03, 1.35721859e-03, 6.33657340e-03,\n",
       "        2.13651600e-03, 4.62175343e-03, 5.67803971e-03, 1.62568290e-03,\n",
       "        1.02039401e-03, 4.17613297e-03, 2.05822654e-03, 4.70798101e-03,\n",
       "        1.00473697e-02, 1.12495118e-02, 1.35682489e-03, 5.74768131e-03,\n",
       "        5.85008078e-03, 1.49741508e-03, 2.63868773e-03, 7.41880451e-03,\n",
       "        1.85288251e-03, 2.57751753e-03, 5.44011507e-03, 1.49638897e-03,\n",
       "        5.17660699e-03, 6.48042573e-03, 2.41681307e-03, 7.36425637e-03,\n",
       "        6.01976335e-03, 1.71997474e-03, 3.70896517e-03, 2.31507383e-03,\n",
       "        2.87038195e-03, 2.53027980e-03, 7.23889885e-03, 1.35575726e-03,\n",
       "        3.05939575e-03, 5.46431150e-03, 1.72067304e-03, 2.72853672e-03,\n",
       "        3.54376568e-03, 3.18766040e-03, 8.31902551e-03, 6.24809184e-03,\n",
       "        3.12429189e-03, 9.43492181e-03, 7.76147047e-03, 3.92920833e-03,\n",
       "        4.67390004e-03, 6.78418072e-03, 5.76218862e-03, 4.04608620e-03,\n",
       "        4.06321026e-03, 1.49647176e-03, 3.50901755e-03, 3.28654714e-03,\n",
       "        2.96653778e-03, 4.00002012e-03, 5.41979097e-03, 1.85476934e-03,\n",
       "        3.19931815e-03, 6.94507701e-03, 3.52108110e-03, 3.71964073e-03,\n",
       "        8.14985466e-03, 1.67379727e-03, 4.03045304e-03, 4.03114404e-03,\n",
       "        3.05972636e-03, 2.52913129e-03, 7.45623066e-03, 2.09780686e-03,\n",
       "        6.06571920e-03, 3.40633054e-03, 4.70679596e-03, 8.94471683e-04,\n",
       "        3.49863652e-03, 2.24519292e-03, 7.47157049e-04, 5.98589624e-03,\n",
       "        4.07060806e-03, 2.82778387e-03, 7.92502157e-03, 1.83240347e-03,\n",
       "        1.06467639e-02, 9.57914767e-03, 4.12797545e-03, 9.30275627e-03,\n",
       "        4.02111087e-03, 1.46943238e-03, 3.72067089e-03, 6.04637179e-03,\n",
       "        1.09506103e-03, 3.57786270e-03, 4.16574165e-03, 2.31468240e-03,\n",
       "        2.33167691e-03, 2.56106447e-03, 1.26500827e-03, 3.92951659e-03,\n",
       "        6.11891299e-03, 1.46952911e-03, 1.04876693e-02, 3.82680319e-03,\n",
       "        1.72006329e-03, 2.57692581e-03, 5.74751374e-03, 3.43997427e-03,\n",
       "        1.93852328e-03, 3.68685270e-03, 1.16360771e-03, 2.92651064e-03,\n",
       "        1.78765864e-03, 9.00239474e-03, 3.89892448e-03, 6.74169324e-03,\n",
       "        1.78821828e-03, 5.98001803e-03, 6.79397220e-03, 1.35636786e-03,\n",
       "        5.49165203e-03, 1.19932359e-02, 3.16238020e-03, 8.80074285e-03,\n",
       "        1.19996429e-02, 3.44059208e-03, 1.17544501e-02, 7.60532581e-03,\n",
       "        1.15476881e-02, 2.60758431e-03, 7.72033469e-03, 7.54655461e-03,\n",
       "        8.33295825e-03, 8.01484294e-03, 2.41580296e-03, 7.19353252e-03,\n",
       "        1.11710263e-02, 7.18449276e-03, 7.44564114e-03, 4.35515563e-03,\n",
       "        2.92432328e-03, 3.48665369e-03, 8.24690312e-03, 2.92600517e-03,\n",
       "        1.01316287e-02, 6.04693160e-03, 1.95946445e-03, 4.48867797e-03,\n",
       "        4.44510810e-03, 1.26564879e-03, 2.24550293e-03, 5.49183780e-03,\n",
       "        1.72026844e-03, 1.85414324e-03, 5.40378765e-03, 5.85259535e-03,\n",
       "        4.26993902e-03, 5.56837496e-03, 8.84605627e-03, 5.30657457e-03,\n",
       "        2.41689986e-03, 2.99331312e-03, 7.35815484e-03, 3.84797584e-03,\n",
       "        8.94789846e-04, 1.71998007e-03, 6.65448603e-03, 3.12310121e-03,\n",
       "        4.43697302e-03, 3.96436268e-03, 2.42852073e-03, 4.91529097e-03,\n",
       "        4.70666217e-03, 2.80024801e-03, 2.57727335e-03, 3.86852929e-03,\n",
       "        7.65375459e-03, 3.18765737e-03, 1.22810972e-02, 1.93882367e-03,\n",
       "        1.36383193e-02, 1.26406647e-03, 3.09887012e-03, 4.01864839e-03,\n",
       "        2.75692139e-03, 5.67888777e-03, 3.49752764e-03, 5.91195529e-03,\n",
       "        3.07311445e-03, 1.93895614e-03, 6.65155776e-03, 2.72669071e-03,\n",
       "        2.31376869e-03, 6.32418766e-03, 2.60857133e-03, 5.42578716e-03,\n",
       "        5.36713393e-03, 1.93966938e-03, 6.04663831e-03, 6.14465901e-03,\n",
       "        3.77359952e-03, 9.79443773e-04, 6.59016765e-03, 1.62492475e-03,\n",
       "        2.36662315e-03, 6.22881180e-03, 1.26572454e-03, 3.44215530e-03,\n",
       "        5.40373436e-03, 2.65256859e-03, 4.01977785e-03, 4.44552028e-03,\n",
       "        1.35650834e-03, 7.52045716e-03, 6.61476288e-03, 1.26515929e-03,\n",
       "        2.15395428e-03, 2.60765666e-03, 2.99303571e-03, 2.40049767e-03,\n",
       "        4.97994381e-03, 2.99212477e-03, 3.82582338e-03, 3.54421235e-03,\n",
       "        1.59949671e-03, 4.11803696e-03, 9.02446213e-03, 9.79997734e-04,\n",
       "        5.60675234e-03, 5.99933468e-03, 2.05872670e-03, 4.22319718e-03,\n",
       "        2.75716347e-03, 2.33273516e-03, 4.49080194e-03, 4.14758578e-03,\n",
       "        2.56181635e-03, 4.70778246e-03, 8.54615222e-03, 5.94645039e-03,\n",
       "        2.56145153e-03, 6.72311084e-03, 2.03995189e-03, 4.96365764e-03,\n",
       "        5.46325875e-03, 7.48277862e-04, 4.95626731e-03, 3.49859291e-03,\n",
       "        7.49016643e-04, 2.24470016e-03, 3.65585212e-03, 1.95982939e-03,\n",
       "        3.49815953e-03, 4.31718654e-03, 1.32699245e-03, 6.62419274e-03,\n",
       "        4.30869857e-03, 2.63894807e-03, 2.63896260e-03, 6.24875277e-03,\n",
       "        2.87022906e-03, 4.71705905e-03, 1.93925143e-03, 2.57623347e-03,\n",
       "        4.66638426e-03, 8.40505487e-03, 4.70774816e-03, 8.01398981e-03,\n",
       "        1.75337030e-02, 1.62568795e-03, 4.49915628e-03, 1.99710348e-02,\n",
       "        3.05980098e-03, 1.84460410e-02, 2.03424919e-02, 2.89788486e-03,\n",
       "        1.59702984e-02, 7.86417483e-03, 3.00699647e-03, 3.38249321e-03,\n",
       "        1.07585477e-02, 4.51673086e-03, 1.95295950e-02, 8.79582866e-03,\n",
       "        5.20032218e-03, 7.05381577e-03, 7.49920820e-03, 1.74389278e-03,\n",
       "        4.48990355e-03, 4.77447864e-03, 2.33188466e-03, 6.21617014e-03,\n",
       "        3.86655199e-03, 4.00233876e-04, 4.35431463e-03, 3.26141056e-03,\n",
       "        8.00443155e-04, 7.99572547e-04, 8.28012075e-03, 1.95954710e-03,\n",
       "        3.07248396e-03, 3.28661985e-03, 9.79160886e-04, 3.26263264e-03,\n",
       "        5.49195936e-03, 1.93944791e-03, 2.60765658e-03, 5.62219335e-03,\n",
       "        4.89204970e-04, 8.93990221e-04, 1.03416970e-02, 2.33227719e-03,\n",
       "        2.44937066e-03, 7.96007623e-03, 2.40011221e-03, 3.48705790e-03,\n",
       "        1.01667140e-02, 2.13489026e-03, 9.81778914e-03, 8.21217499e-03,\n",
       "        2.44974050e-03, 3.28528466e-03, 3.03368511e-03, 2.72879891e-03,\n",
       "        1.95984888e-03, 1.45544499e-02, 3.74260974e-03, 5.78403727e-03,\n",
       "        2.41235028e-02, 7.16661000e-03, 3.49742933e-03, 1.14020071e-02,\n",
       "        8.95002684e-04, 2.03898793e-03, 1.11100696e-02, 1.59950266e-03,\n",
       "        7.17314461e-03, 2.31517674e-03, 4.90291391e-04, 3.05880674e-03,\n",
       "        2.96655378e-03, 2.92557488e-03, 5.46217804e-03, 4.70677566e-03,\n",
       "        9.79510847e-04, 4.07901585e-03, 1.91277464e-02, 2.60800404e-03,\n",
       "        1.53574753e-02, 2.08562737e-02, 1.85498528e-03, 1.69873307e-02,\n",
       "        9.56356757e-03, 3.74204889e-03, 7.59990344e-03, 6.85267849e-03,\n",
       "        5.08312337e-03, 5.67782478e-03, 9.68346764e-03, 6.27396748e-03,\n",
       "        6.14617844e-03, 1.02644029e-02, 5.31431237e-03, 5.81048078e-03,\n",
       "        2.03308517e-02, 2.16005987e-03, 9.06198667e-03, 9.73592080e-03,\n",
       "        4.30831341e-03, 2.64507453e-03, 9.38033467e-03, 1.32536784e-03,\n",
       "        5.45486688e-03, 6.82827398e-03, 2.60741906e-03, 2.57664426e-03,\n",
       "        4.58624197e-03, 2.03863714e-03, 3.26157430e-03, 9.70746738e-03,\n",
       "        2.89837822e-03, 3.54398635e-03, 2.78587298e-03, 8.95324805e-04,\n",
       "        5.20029812e-03, 2.60756521e-03, 8.44713225e-03, 3.68819711e-03,\n",
       "        2.87113283e-03, 3.26191053e-03, 1.78923174e-03, 6.97449868e-03,\n",
       "        1.16604098e-03, 3.63375712e-03, 3.03146842e-03, 1.35708492e-03,\n",
       "        4.63088892e-03, 7.18309891e-03, 2.05922249e-03, 4.31606222e-03,\n",
       "        1.29301174e-02, 5.46363582e-03, 3.21632276e-03, 5.03547983e-03,\n",
       "        3.78818171e-03, 9.10784178e-03, 3.13675958e-03, 2.44968216e-03,\n",
       "        3.05926186e-03, 4.06904446e-03, 1.09536573e-03, 2.85615125e-03,\n",
       "        3.28616993e-03, 2.94010907e-03, 3.48726053e-03, 1.05382780e-02,\n",
       "        3.84772769e-03, 1.03665280e-02, 1.38766475e-02, 3.93070613e-03,\n",
       "        8.30965256e-03, 1.09364109e-02, 1.32592879e-03, 4.11784730e-03,\n",
       "        2.24264289e-02, 2.71283406e-03, 1.53931564e-02, 8.33363511e-03,\n",
       "        3.81624755e-03, 9.20886304e-03, 5.23808454e-03, 2.13533684e-03,\n",
       "        3.52123028e-03, 1.25315683e-02, 4.04983350e-03, 2.71411385e-03,\n",
       "        3.13815508e-03, 3.49916276e-03, 3.99968645e-03, 8.44973337e-03,\n",
       "        2.53031735e-03, 8.79665820e-03, 2.29294346e-02, 1.02080544e-03,\n",
       "        7.02105787e-03, 1.10712074e-02, 1.72075657e-03, 4.12823436e-03,\n",
       "        7.42017741e-03, 1.19963534e-03, 2.57714040e-03, 3.00721856e-03,\n",
       "        2.68365648e-03, 4.44541070e-03, 6.39957194e-03, 3.99828540e-04,\n",
       "        4.03021404e-03, 3.81544795e-03, 6.64571870e-03, 2.24622630e-03,\n",
       "        5.03541578e-03, 1.35640288e-03, 4.22431495e-03, 1.24730918e-02,\n",
       "        2.13446602e-03, 7.77128486e-03, 7.17288123e-03, 1.85447112e-03,\n",
       "        5.85135841e-03, 5.95323323e-03, 3.61111759e-03, 3.13664706e-03,\n",
       "        8.86641567e-03, 1.20031838e-03, 3.38198289e-03, 7.55394340e-03,\n",
       "        3.12388262e-03, 2.03957223e-03, 2.63830462e-03, 3.31035508e-03,\n",
       "        2.96737373e-03, 6.52473911e-03, 2.78623257e-03, 4.60505812e-03,\n",
       "        6.70453818e-03, 1.72105608e-03, 4.12874024e-03, 5.88452691e-03,\n",
       "        4.89267024e-04, 4.51756504e-03, 8.38099127e-03, 2.52948833e-03,\n",
       "        2.85664166e-03, 4.77426900e-03, 7.48201294e-04, 5.41800087e-03,\n",
       "        5.42553409e-03, 1.78840500e-03, 2.65282366e-03, 5.30662670e-03,\n",
       "        2.72813810e-03, 1.72001907e-03, 5.58562415e-03, 4.11811108e-03,\n",
       "        3.13680842e-03, 5.06020137e-03, 1.02028187e-03, 7.52142351e-03,\n",
       "        5.98633913e-03, 7.17203561e-03, 4.31766827e-03, 1.05571832e-02,\n",
       "        3.19927349e-03, 3.92980052e-03, 5.99468775e-03]),\n",
       " 'mean_score_time': array([0.00259876, 0.00580101, 0.00559983, 0.00319891, 0.00479975,\n",
       "        0.00479641, 0.00220056, 0.00299892, 0.00439997, 0.00239935,\n",
       "        0.00419888, 0.00499864, 0.00239935, 0.00339923, 0.00479808,\n",
       "        0.00280027, 0.00359917, 0.00459857, 0.00339985, 0.00419769,\n",
       "        0.00519924, 0.00499916, 0.00399895, 0.00559845, 0.004     ,\n",
       "        0.00360036, 0.00679874, 0.00259938, 0.00499921, 0.00639882,\n",
       "        0.00259943, 0.00419917, 0.00679803, 0.00239944, 0.00439878,\n",
       "        0.00559988, 0.00280018, 0.00439792, 0.00839891, 0.00300016,\n",
       "        0.00539908, 0.00619836, 0.00299983, 0.00399976, 0.00680065,\n",
       "        0.00360031, 0.00639944, 0.00379744, 0.00279961, 0.00319953,\n",
       "        0.00419941, 0.00300012, 0.00319977, 0.00339818, 0.00260038,\n",
       "        0.00380001, 0.0053978 , 0.00280042, 0.00359807, 0.00779614,\n",
       "        0.0025991 , 0.00379915, 0.00499787, 0.00279994, 0.00399942,\n",
       "        0.00699749, 0.00440149, 0.00379801, 0.00559802, 0.00280004,\n",
       "        0.00379977, 0.00519781, 0.00259876, 0.00379949, 0.0067987 ,\n",
       "        0.0079989 , 0.0045989 , 0.0055975 , 0.00259957, 0.00479908,\n",
       "        0.00859866, 0.00279765, 0.0047986 , 0.00659885, 0.00299959,\n",
       "        0.00479913, 0.00739856, 0.00279961, 0.00519905, 0.00699935,\n",
       "        0.00260043, 0.00499897, 0.00459962, 0.00259953, 0.0033988 ,\n",
       "        0.00420032, 0.00239992, 0.00339999, 0.0033978 , 0.00239911,\n",
       "        0.00359864, 0.00959945, 0.00359883, 0.00379882, 0.00679808,\n",
       "        0.00619993, 0.00339856, 0.00619984, 0.00279784, 0.00379915,\n",
       "        0.00539818, 0.00379963, 0.00619936, 0.0056005 , 0.00440021,\n",
       "        0.00379939, 0.00519857, 0.00299964, 0.00479827, 0.00859776,\n",
       "        0.00319929, 0.00519943, 0.00579805, 0.00299973, 0.00479841,\n",
       "        0.00659781, 0.00239916, 0.00479794, 0.00699973, 0.0033989 ,\n",
       "        0.00439868, 0.00879951, 0.00260038, 0.00519929, 0.00619831,\n",
       "        0.00460033, 0.0047998 , 0.00379887, 0.00399938, 0.00579906,\n",
       "        0.00480022, 0.00239992, 0.00359845, 0.0049994 , 0.00280008,\n",
       "        0.00359859, 0.0047996 , 0.00279989, 0.00419884, 0.00519757,\n",
       "        0.0028007 , 0.00339961, 0.00659981, 0.00500002, 0.006598  ,\n",
       "        0.00559926, 0.00260029, 0.00379834, 0.0057991 , 0.00360003,\n",
       "        0.00379844, 0.00679932, 0.00239892, 0.00439625, 0.00660267,\n",
       "        0.00379982, 0.00399885, 0.00619855, 0.00280027, 0.00419803,\n",
       "        0.00579963, 0.00299954, 0.00479908, 0.00759907, 0.00259943,\n",
       "        0.0039988 , 0.00679936, 0.00400009, 0.00419908, 0.0058002 ,\n",
       "        0.00280104, 0.00319977, 0.00439873, 0.00280066, 0.00339985,\n",
       "        0.00479937, 0.00259976, 0.00459938, 0.00419936, 0.00399923,\n",
       "        0.00359988, 0.00599895, 0.00360017, 0.00399923, 0.005199  ,\n",
       "        0.00299897, 0.00379901, 0.00539856, 0.00259972, 0.00499783,\n",
       "        0.00559945, 0.00279951, 0.00379786, 0.00459862, 0.0046    ,\n",
       "        0.00399785, 0.0059979 , 0.00340052, 0.0053987 , 0.00859928,\n",
       "        0.00299916, 0.00559921, 0.00619845, 0.00399981, 0.00479946,\n",
       "        0.00539894, 0.00299935, 0.00439854, 0.00699859, 0.00340014,\n",
       "        0.00599833, 0.00619926, 0.00299978, 0.00419836, 0.00599675,\n",
       "        0.00279999, 0.00339913, 0.0043973 , 0.00240049, 0.00320005,\n",
       "        0.00459886, 0.0022007 , 0.00380034, 0.00419955, 0.00319886,\n",
       "        0.00359864, 0.00439873, 0.00279975, 0.00339918, 0.00499845,\n",
       "        0.00280013, 0.00359969, 0.00479903, 0.00440083, 0.0045989 ,\n",
       "        0.0075984 , 0.00259986, 0.0037993 , 0.00699754, 0.00279899,\n",
       "        0.00419927, 0.00579867, 0.00360036, 0.00459962, 0.00659976,\n",
       "        0.00299945, 0.00719771, 0.00619798, 0.00259886, 0.0037993 ,\n",
       "        0.00519886, 0.00299854, 0.00419908, 0.00619926, 0.00359945,\n",
       "        0.0039988 , 0.00599923, 0.00339985, 0.00379896, 0.00499873,\n",
       "        0.00340099, 0.0031991 , 0.00479836, 0.00279984, 0.0027998 ,\n",
       "        0.00399799, 0.00280027, 0.00299878, 0.00499892, 0.00259938,\n",
       "        0.00459991, 0.00499907, 0.00340023, 0.00419917, 0.00759878,\n",
       "        0.00260024, 0.00519938, 0.00559845, 0.00299959, 0.00459995,\n",
       "        0.0055995 , 0.00300145, 0.00419931, 0.00559902, 0.00279908,\n",
       "        0.00439897, 0.00519805, 0.00339909, 0.00539808, 0.00679898,\n",
       "        0.00279994, 0.00459781, 0.00659981, 0.0032002 , 0.00399914,\n",
       "        0.00619931, 0.00299983, 0.00479832, 0.00740032, 0.0030004 ,\n",
       "        0.00599933, 0.00639954, 0.0030004 , 0.00359807, 0.00679836,\n",
       "        0.00219917, 0.00299911, 0.00359902, 0.00299964, 0.00379953,\n",
       "        0.00419903, 0.0026    , 0.00399981, 0.00379748, 0.00259976,\n",
       "        0.00399985, 0.00499701, 0.00320015, 0.00419908, 0.00559859,\n",
       "        0.00259938, 0.00359917, 0.00579906, 0.00239925, 0.00379906,\n",
       "        0.00579906, 0.00299783, 0.00439854, 0.00759606, 0.00260053,\n",
       "        0.00379891, 0.00619712, 0.00279875, 0.00439749, 0.00639992,\n",
       "        0.00300021, 0.00379705, 0.00599723, 0.00320125, 0.00399966,\n",
       "        0.00619826, 0.00319853, 0.0047986 , 0.007798  , 0.00319982,\n",
       "        0.00539885, 0.00639906, 0.00259986, 0.00499945, 0.0059998 ,\n",
       "        0.00239983, 0.00359931, 0.00439944, 0.00360022, 0.00339947,\n",
       "        0.00459862, 0.00320063, 0.00359898, 0.00439796, 0.00219936,\n",
       "        0.003198  , 0.00479856, 0.00219951, 0.00379949, 0.00459919,\n",
       "        0.00239973, 0.00499959, 0.00479922, 0.00339975, 0.005199  ,\n",
       "        0.00620022, 0.00339999, 0.00419984, 0.00539937, 0.00259981,\n",
       "        0.00439982, 0.00539942, 0.00320063, 0.00439901, 0.00599961,\n",
       "        0.00279999, 0.00439682, 0.00619893, 0.00339971, 0.00439992,\n",
       "        0.0053988 , 0.00319972, 0.00499949, 0.00680032, 0.00280027,\n",
       "        0.00439901, 0.00739942, 0.00320072, 0.00519915, 0.00639949,\n",
       "        0.00279994, 0.00339971, 0.00440049, 0.00260043, 0.00639973,\n",
       "        0.00519938, 0.00239959, 0.00299954, 0.00379767, 0.00319982,\n",
       "        0.00379696, 0.00499821, 0.0026    , 0.00399919, 0.00459976,\n",
       "        0.00300064, 0.00459847, 0.00539927, 0.00279956, 0.00699863,\n",
       "        0.00539827, 0.00279942, 0.00459881, 0.01039934, 0.00479984,\n",
       "        0.00559807, 0.00519896, 0.00299969, 0.00439858, 0.00639958,\n",
       "        0.00340014, 0.00519791, 0.0071991 , 0.00300012, 0.00439935,\n",
       "        0.00719919, 0.00319996, 0.00459876, 0.00699954, 0.00380054,\n",
       "        0.00479889, 0.00719938, 0.00299973, 0.00459981, 0.00599885,\n",
       "        0.00279946, 0.00420032, 0.00439978, 0.00299983, 0.00499983,\n",
       "        0.00439992, 0.00259953, 0.00360026, 0.00519905, 0.00280032,\n",
       "        0.00380011, 0.00599966, 0.00240006, 0.00400023, 0.0051991 ,\n",
       "        0.00320001, 0.00379972, 0.00459833, 0.00260153, 0.00399966,\n",
       "        0.00559821, 0.0029995 , 0.00379977, 0.00559907, 0.00259991,\n",
       "        0.00359998, 0.00599704, 0.00299954, 0.0043992 , 0.00599923,\n",
       "        0.00300026, 0.00419917, 0.00599909, 0.00399971, 0.0047997 ,\n",
       "        0.00599847, 0.00319934, 0.00439768, 0.00619907, 0.00320001,\n",
       "        0.00479889, 0.00579944, 0.00239944, 0.00379939, 0.00519996,\n",
       "        0.00260034, 0.00379996, 0.00499911, 0.00240049, 0.00420065,\n",
       "        0.00519929, 0.00280023, 0.00299954, 0.00459971, 0.00299983,\n",
       "        0.00379915, 0.00579953, 0.00359969, 0.00519915, 0.00579958,\n",
       "        0.00280004, 0.0049994 , 0.00559802, 0.00419984, 0.00520062,\n",
       "        0.0053988 , 0.00359945, 0.00459833, 0.0073997 , 0.00299978,\n",
       "        0.00419927, 0.0055985 , 0.0037991 , 0.00479903, 0.00619888,\n",
       "        0.00319915, 0.00459905, 0.00639935, 0.00300002, 0.00679927,\n",
       "        0.00619726, 0.00239954, 0.00439825, 0.00619888, 0.00279894,\n",
       "        0.00419908, 0.00579786, 0.0026001 , 0.005199  , 0.00559897,\n",
       "        0.00299921, 0.00439939, 0.00399833, 0.00279994, 0.00339956,\n",
       "        0.00439973, 0.00279989, 0.00299988, 0.00499864, 0.0027998 ,\n",
       "        0.00359955, 0.00499778, 0.00239921, 0.00359917, 0.00439711,\n",
       "        0.0021997 , 0.00419931, 0.00519867, 0.00239921, 0.00519843,\n",
       "        0.00599928, 0.00319924, 0.00501132, 0.00519953, 0.00259991,\n",
       "        0.00539956, 0.00559916, 0.00280008, 0.00599799, 0.00639873,\n",
       "        0.00379972, 0.00379896, 0.00819869, 0.00280061, 0.00439878,\n",
       "        0.00679855, 0.00359969, 0.00519791, 0.0089994 , 0.00600142,\n",
       "        0.00539861, 0.00779562, 0.00279889, 0.00399981, 0.00640039,\n",
       "        0.0027997 , 0.00360031, 0.00419955, 0.00219922, 0.00379944,\n",
       "        0.00439858, 0.00360036, 0.0033989 , 0.00460033, 0.00279984,\n",
       "        0.00599837, 0.00499892, 0.00279989, 0.00419974, 0.00479732,\n",
       "        0.00299959, 0.00339842, 0.00479827, 0.00259967, 0.00539341,\n",
       "        0.00659752, 0.00259914, 0.00379882, 0.00639777, 0.00279965,\n",
       "        0.00419946, 0.00539947, 0.00299973, 0.0043982 , 0.00679917,\n",
       "        0.00299978, 0.0041995 , 0.00699773, 0.00219998, 0.00379882,\n",
       "        0.00619826, 0.00560021, 0.004598  , 0.00739875, 0.00279913,\n",
       "        0.00560107, 0.00739923, 0.00300074, 0.00559993, 0.00599866,\n",
       "        0.00339937, 0.00359921, 0.00459919, 0.00280008, 0.00339823,\n",
       "        0.00459943, 0.00300002, 0.00359912, 0.0041985 , 0.00240002,\n",
       "        0.00359941, 0.00479918, 0.00320067, 0.00420079, 0.00499964,\n",
       "        0.0023994 , 0.00379896, 0.00539894, 0.00259867, 0.00499754,\n",
       "        0.00579777, 0.00359912, 0.00499887, 0.00539951, 0.00259995,\n",
       "        0.00419993, 0.00539923, 0.00259943, 0.00459876, 0.0063992 ,\n",
       "        0.00279951, 0.00399961, 0.0061986 , 0.00319977, 0.00399919,\n",
       "        0.00599875, 0.00320044, 0.00459785, 0.00739999, 0.00319972,\n",
       "        0.00440016, 0.00659962, 0.00360036, 0.00419984, 0.00559802]),\n",
       " 'std_score_time': array([4.92901037e-04, 3.48353073e-03, 1.62486602e-03, 1.16707238e-03,\n",
       "        3.12633713e-03, 1.71783040e-03, 3.99971548e-04, 6.14361702e-07,\n",
       "        8.01946306e-04, 4.89863926e-04, 9.79696060e-04, 6.32938121e-04,\n",
       "        4.89667897e-04, 4.89317657e-04, 7.48367028e-04, 4.00210691e-04,\n",
       "        4.89979613e-04, 4.89162589e-04, 1.02025429e-03, 9.80427171e-04,\n",
       "        4.00400517e-04, 4.04654852e-03, 6.33468131e-04, 1.20060466e-03,\n",
       "        1.26561108e-03, 4.90274055e-04, 3.65757953e-03, 4.89514524e-04,\n",
       "        1.09480100e-03, 4.90446909e-04, 4.90329969e-04, 4.00472890e-04,\n",
       "        1.60086228e-03, 4.90076775e-04, 1.02046016e-03, 4.90875625e-04,\n",
       "        7.49590066e-04, 4.89455386e-04, 3.32240879e-03, 5.56082906e-07,\n",
       "        1.85544810e-03, 9.76903495e-04, 1.26519195e-06, 6.32485754e-04,\n",
       "        7.48268010e-04, 1.74320900e-03, 6.31270375e-03, 4.00497871e-04,\n",
       "        7.48417553e-04, 4.00329016e-04, 9.80416526e-04, 6.32711515e-04,\n",
       "        4.04026080e-04, 4.90857986e-04, 4.90719359e-04, 9.79783494e-04,\n",
       "        4.89980796e-04, 7.49654916e-04, 1.20074844e-03, 3.42802936e-03,\n",
       "        4.90154658e-04, 1.60076625e-03, 1.09580107e-03, 7.48188399e-04,\n",
       "        6.33089719e-04, 2.09858001e-03, 2.79924935e-03, 7.48264850e-04,\n",
       "        8.00589132e-04, 7.48723799e-04, 1.16652313e-03, 4.00047774e-04,\n",
       "        4.90174173e-04, 4.00328959e-04, 2.13549822e-03, 9.50834132e-03,\n",
       "        4.90801480e-04, 4.91557515e-04, 8.00180499e-04, 1.72141031e-03,\n",
       "        6.71126679e-03, 7.49107448e-04, 7.48508002e-04, 4.88932383e-04,\n",
       "        4.86280395e-07, 7.47755242e-04, 2.41632756e-03, 3.99995060e-04,\n",
       "        2.03961016e-03, 1.99956904e-03, 4.89493352e-04, 3.09880865e-03,\n",
       "        8.00276293e-04, 1.19981792e-03, 4.90058184e-04, 4.01330685e-04,\n",
       "        4.89492934e-04, 4.89376575e-04, 4.89517497e-04, 4.90447024e-04,\n",
       "        7.99454710e-04, 8.70830948e-03, 1.19850030e-03, 7.48481304e-04,\n",
       "        5.11412849e-03, 6.91078388e-03, 4.88987580e-04, 1.72052891e-03,\n",
       "        3.99595030e-04, 4.00758396e-04, 4.90154890e-04, 2.13514038e-03,\n",
       "        3.12409935e-03, 4.88660300e-04, 1.35660012e-03, 7.48570576e-04,\n",
       "        3.99184779e-04, 6.32937550e-04, 1.16631882e-03, 3.72079670e-03,\n",
       "        4.00210151e-04, 7.48710932e-04, 7.48443752e-04, 3.50402318e-07,\n",
       "        2.13521209e-03, 2.24473834e-03, 4.90116312e-04, 1.60062649e-03,\n",
       "        6.32937593e-04, 1.02053568e-03, 4.89612219e-04, 4.11680278e-03,\n",
       "        4.89261122e-04, 1.16628584e-03, 7.49628955e-04, 3.26013948e-03,\n",
       "        1.46941215e-03, 3.99788605e-04, 1.09545307e-03, 3.05931159e-03,\n",
       "        1.16722683e-03, 4.89979636e-04, 4.89792659e-04, 1.26361318e-03,\n",
       "        4.00352677e-04, 4.90291113e-04, 7.48710598e-04, 7.48876463e-04,\n",
       "        1.47009371e-03, 1.16474309e-03, 4.00306229e-04, 4.90369420e-04,\n",
       "        4.22532617e-03, 4.04969229e-03, 2.93942148e-03, 4.89590239e-04,\n",
       "        4.90446839e-04, 4.01427864e-04, 9.80212406e-04, 2.24522258e-03,\n",
       "        3.99923623e-04, 3.12425811e-03, 4.90797426e-04, 4.89362956e-04,\n",
       "        7.98021505e-04, 2.13591764e-03, 1.14440918e-06, 4.01189192e-04,\n",
       "        3.99971179e-04, 7.48659887e-04, 7.48609518e-04, 4.52367448e-07,\n",
       "        3.99089782e-04, 1.35625561e-03, 8.00466999e-04, 8.71451706e-07,\n",
       "        7.48943165e-04, 1.99954516e-03, 7.47173205e-04, 3.99923964e-04,\n",
       "        7.48865790e-04, 3.99613448e-04, 4.88987790e-04, 4.00281020e-04,\n",
       "        4.89882362e-04, 1.72116631e-03, 4.89045719e-04, 3.19991113e-03,\n",
       "        9.80036471e-04, 2.09723855e-03, 4.90174173e-04, 2.09773851e-03,\n",
       "        2.24480207e-03, 9.29526743e-07, 3.99208553e-04, 2.61435512e-06,\n",
       "        3.99852094e-04, 1.95803366e-03, 4.89979451e-04, 2.09564729e-03,\n",
       "        4.90038205e-04, 4.00782270e-04, 3.99878683e-04, 4.89687780e-04,\n",
       "        3.72054533e-03, 6.33089413e-04, 2.00033228e-03, 4.89921056e-04,\n",
       "        1.49721124e-03, 3.77309403e-03, 6.32410027e-04, 2.80108796e-03,\n",
       "        3.98493655e-04, 2.09751118e-03, 4.00948819e-04, 4.90506033e-04,\n",
       "        6.33239143e-04, 4.88948027e-04, 6.31883264e-04, 1.02009476e-03,\n",
       "        2.28181758e-03, 3.99995117e-04, 6.57274664e-07, 7.46787983e-04,\n",
       "        1.13242551e-06, 7.48137276e-04, 4.90759543e-04, 4.89573092e-04,\n",
       "        4.90098571e-04, 7.47760259e-04, 7.99656810e-04, 3.99663265e-04,\n",
       "        2.13699377e-03, 3.99566589e-04, 9.79862758e-04, 4.90918905e-04,\n",
       "        4.90448369e-04, 7.48583236e-04, 4.91207623e-04, 1.98332160e-06,\n",
       "        4.00019104e-04, 4.90115755e-04, 7.47347573e-04, 2.24481904e-03,\n",
       "        1.01947766e-03, 2.41677371e-03, 4.89999140e-04, 4.00591024e-04,\n",
       "        1.67297099e-03, 4.00162965e-04, 3.99828455e-04, 7.48038185e-04,\n",
       "        8.00132879e-04, 8.00454643e-04, 7.99704430e-04, 6.32108520e-04,\n",
       "        6.91034920e-03, 9.79269472e-04, 4.90446978e-04, 4.00115324e-04,\n",
       "        7.48163437e-04, 6.32108175e-04, 3.99447868e-04, 4.00476439e-04,\n",
       "        1.85481044e-03, 1.08106461e-06, 6.31806834e-04, 7.99656184e-04,\n",
       "        3.99947490e-04, 1.72190381e-06, 1.85499063e-03, 3.99112844e-04,\n",
       "        4.00761828e-04, 4.00114074e-04, 4.00329840e-04, 1.15232906e-06,\n",
       "        1.16725103e-03, 6.32560758e-04, 1.67356934e-03, 4.90096159e-04,\n",
       "        2.24457697e-03, 9.84180805e-07, 1.35650877e-03, 9.79832030e-04,\n",
       "        3.87815060e-03, 4.90408099e-04, 1.93920197e-03, 1.74382707e-03,\n",
       "        7.29420592e-07, 7.99429955e-04, 4.90174822e-04, 1.50033309e-06,\n",
       "        3.99685162e-04, 8.00132808e-04, 7.48596743e-04, 4.89376853e-04,\n",
       "        3.99686982e-04, 1.35737310e-03, 2.33294365e-03, 1.16650680e-03,\n",
       "        3.99685702e-04, 8.00801715e-04, 1.01928126e-03, 9.80124594e-04,\n",
       "        6.33391102e-04, 9.79335690e-04, 6.32711367e-04, 3.99781781e-04,\n",
       "        1.96029694e-03, 5.56082906e-07, 3.03346511e-03, 1.20090650e-03,\n",
       "        7.00804637e-07, 4.91034802e-04, 1.83308477e-03, 4.00424724e-04,\n",
       "        5.51978917e-07, 8.00121081e-04, 3.56832255e-07, 7.48353948e-04,\n",
       "        3.99948144e-04, 4.90117193e-04, 1.26549824e-03, 3.99926750e-04,\n",
       "        4.90214258e-04, 6.32410566e-04, 2.76812082e-06, 9.79403762e-04,\n",
       "        1.93872532e-03, 7.99441400e-04, 4.90485780e-04, 4.90471280e-04,\n",
       "        1.72113865e-03, 4.89940687e-04, 4.00473997e-04, 7.48519828e-04,\n",
       "        2.34766648e-06, 7.99799328e-04, 4.75800257e-03, 4.90838383e-04,\n",
       "        4.00527216e-04, 1.93758437e-03, 7.48634633e-04, 4.90489535e-04,\n",
       "        4.89843684e-04, 6.33089341e-04, 4.01863126e-04, 1.14242063e-06,\n",
       "        4.01617070e-04, 6.32635968e-04, 1.47063875e-03, 7.49819292e-04,\n",
       "        7.49083211e-04, 2.13628860e-03, 3.99831753e-04, 1.85503682e-03,\n",
       "        4.90739194e-04, 4.90096576e-04, 1.54932761e-03, 6.32636105e-04,\n",
       "        4.89862836e-04, 8.00740965e-04, 4.90935676e-04, 1.74348795e-03,\n",
       "        4.89998815e-04, 7.98941149e-04, 3.99900119e-04, 7.99322150e-04,\n",
       "        1.01988916e-03, 4.00329442e-04, 4.01689575e-04, 4.01688896e-04,\n",
       "        3.99661132e-04, 7.48914487e-04, 4.89473903e-04, 4.90038182e-04,\n",
       "        3.03349632e-03, 4.00710741e-04, 4.89862488e-04, 1.93953643e-03,\n",
       "        1.16648225e-03, 1.85525773e-03, 4.00734175e-04, 4.89960067e-04,\n",
       "        4.89960183e-04, 7.99870681e-04, 4.90213145e-04, 9.80328568e-04,\n",
       "        4.89823956e-04, 1.08106461e-06, 7.48073511e-04, 4.89279297e-04,\n",
       "        4.00519751e-04, 2.33244505e-03, 4.89473136e-04, 4.89358357e-04,\n",
       "        3.99756556e-04, 6.32409739e-04, 7.48621470e-04, 3.99852151e-04,\n",
       "        4.90700732e-04, 4.89804325e-04, 4.01521391e-04, 1.93920197e-03,\n",
       "        4.90096228e-04, 3.99565793e-04, 4.89902334e-04, 4.89784999e-04,\n",
       "        4.90271783e-04, 3.07267648e-03, 4.00091308e-04, 4.89668245e-04,\n",
       "        9.77225377e-07, 3.99423453e-04, 4.01022233e-04, 7.48250897e-04,\n",
       "        8.93511344e-04, 4.89629234e-04, 6.32938804e-04, 4.89648459e-04,\n",
       "        6.32334879e-04, 1.35721273e-03, 8.00646102e-04, 7.48022619e-04,\n",
       "        6.03327069e-03, 4.89005980e-04, 4.00854642e-04, 1.85461000e-03,\n",
       "        5.27703043e-03, 4.62157802e-03, 2.80053286e-03, 7.48520208e-04,\n",
       "        6.32334843e-04, 4.90175100e-04, 1.01994552e-03, 1.49795005e-03,\n",
       "        2.13475200e-03, 1.93914324e-03, 5.95569420e-07, 4.90330340e-04,\n",
       "        1.46985467e-03, 9.79588714e-04, 8.00979196e-04, 6.32937798e-04,\n",
       "        1.59971812e-03, 7.48392555e-04, 3.99783942e-04, 8.31393994e-07,\n",
       "        4.89590959e-04, 3.37174788e-07, 3.99327944e-04, 9.81496414e-04,\n",
       "        8.00967364e-04, 6.32485251e-04, 3.52123011e-03, 8.00061231e-04,\n",
       "        4.89921567e-04, 4.90680510e-04, 1.46943821e-03, 4.00233336e-04,\n",
       "        4.00281361e-04, 1.54840550e-03, 4.89668663e-04, 5.30983387e-07,\n",
       "        4.00115153e-04, 1.46965230e-03, 7.48468602e-04, 4.89552513e-04,\n",
       "        4.91366257e-04, 5.91739352e-07, 4.90098038e-04, 6.28991411e-07,\n",
       "        7.48162570e-04, 4.89337471e-04, 4.89940548e-04, 4.90545878e-04,\n",
       "        1.99849688e-03, 6.31957900e-04, 4.90641445e-04, 6.32259094e-04,\n",
       "        6.33012903e-04, 7.47971858e-04, 6.32487653e-04, 1.26561171e-03,\n",
       "        1.16578702e-03, 1.09541026e-03, 7.48074392e-04, 7.99751808e-04,\n",
       "        4.00925221e-04, 1.47043117e-03, 7.49730853e-04, 3.99065744e-04,\n",
       "        4.90175448e-04, 4.00520886e-04, 3.99924590e-04, 1.20000870e-03,\n",
       "        7.48787570e-04, 2.60718113e-03, 4.89123106e-04, 1.46900363e-03,\n",
       "        1.60090949e-03, 4.00186079e-04, 8.92080638e-07, 1.01993616e-03,\n",
       "        8.94309055e-04, 3.99804723e-04, 1.16593440e-03, 1.02017904e-03,\n",
       "        2.48173566e-03, 9.80017177e-04, 7.48723480e-04, 1.78880474e-03,\n",
       "        1.74441305e-03, 2.48226207e-03, 2.04109194e-03, 4.90330479e-04,\n",
       "        1.01985159e-03, 1.74400233e-03, 2.24538396e-03, 6.32183953e-04,\n",
       "        1.16609927e-03, 1.02004815e-03, 4.00140583e-04, 2.13589509e-03,\n",
       "        7.48775706e-04, 7.48315601e-04, 4.90626680e-04, 1.49692470e-03,\n",
       "        6.33768355e-04, 3.76313566e-03, 1.47100251e-03, 4.90290788e-04,\n",
       "        7.99883334e-04, 4.00663937e-04, 3.99424136e-04, 4.00519694e-04,\n",
       "        4.00307223e-04, 4.90193993e-04, 1.93875035e-03, 4.90038089e-04,\n",
       "        6.31279608e-04, 1.95987321e-03, 9.46494734e-07, 4.00519666e-04,\n",
       "        4.89727859e-04, 1.01929061e-03, 4.00734487e-04, 1.32831462e-06,\n",
       "        1.26519703e-03, 4.00448470e-04, 8.00324296e-04, 1.44159474e-06,\n",
       "        4.89590657e-04, 4.90272803e-04, 4.88950840e-04, 4.00639375e-04,\n",
       "        4.00042914e-04, 4.00449662e-04, 4.89784907e-04, 2.22685901e-03,\n",
       "        6.32636543e-04, 3.99518422e-04, 9.57184198e-04, 4.00377069e-04,\n",
       "        4.89551909e-04, 2.49708135e-03, 4.89317704e-04, 4.00710457e-04,\n",
       "        3.52123017e-03, 7.99274886e-04, 1.60048009e-03, 7.47946557e-04,\n",
       "        5.91286647e-03, 4.00501050e-04, 8.00336779e-04, 3.12196572e-03,\n",
       "        4.90018438e-04, 1.46985390e-03, 3.79462154e-03, 4.56336140e-03,\n",
       "        2.33316047e-03, 1.71915090e-03, 7.48863756e-04, 6.32562038e-04,\n",
       "        1.35632678e-03, 3.99210660e-04, 8.00109498e-04, 4.00295817e-04,\n",
       "        3.99566163e-04, 7.48646897e-04, 1.02002070e-03, 1.62504848e-03,\n",
       "        4.90271574e-04, 7.99799669e-04, 4.00948563e-04, 2.28119008e-03,\n",
       "        8.93510224e-04, 3.99900318e-04, 9.80836683e-04, 3.99292190e-04,\n",
       "        1.20064529e-06, 4.90466273e-04, 3.98804934e-04, 4.90233344e-04,\n",
       "        1.35428247e-03, 2.72567661e-03, 4.90389233e-04, 7.47973029e-04,\n",
       "        1.85486758e-03, 4.00376416e-04, 7.49174388e-04, 4.90465995e-04,\n",
       "        1.09427846e-03, 4.89804627e-04, 1.16608973e-03, 6.32410142e-04,\n",
       "        4.00305433e-04, 3.16215452e-03, 4.00019616e-04, 4.01668263e-04,\n",
       "        7.50152249e-04, 3.07177317e-03, 4.88698062e-04, 1.35724676e-03,\n",
       "        3.99995145e-04, 1.19869717e-03, 1.49731914e-03, 6.33012885e-04,\n",
       "        4.90039527e-04, 1.09523553e-03, 1.35641003e-03, 1.01975812e-03,\n",
       "        4.90447279e-04, 1.16660489e-03, 4.90719706e-04, 4.90447604e-04,\n",
       "        6.32786786e-04, 7.99930403e-04, 3.99852606e-04, 4.90194967e-04,\n",
       "        4.90856852e-04, 3.99614131e-04, 3.99637692e-04, 3.99424563e-04,\n",
       "        8.12024420e-07, 4.89823840e-04, 1.16667060e-03, 4.90801411e-04,\n",
       "        4.89616468e-04, 1.54803514e-03, 1.59959824e-03, 4.89455665e-04,\n",
       "        2.00035592e-03, 4.89551909e-04, 4.90368725e-04, 3.99852293e-04,\n",
       "        4.88519541e-04, 4.89940455e-04, 4.89610571e-04, 4.90622139e-04,\n",
       "        3.99470340e-04, 6.32485488e-04, 4.00568144e-04, 3.99971179e-04,\n",
       "        6.32862821e-04, 6.32335706e-04, 3.99996310e-04, 4.90817161e-04,\n",
       "        1.85504185e-03, 4.00353415e-04, 8.02684091e-04, 4.89551561e-04,\n",
       "        8.00728832e-04, 3.99783202e-04, 4.89161799e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.83168317, 0.83809524, 0.84313725, 0.84313725, 0.85714286,\n",
       "        0.83809524, 0.85436893, 0.85714286, 0.87378641, 0.85148515,\n",
       "        0.85436893, 0.81188119, 0.85148515, 0.85714286, 0.83495146,\n",
       "        0.84313725, 0.84313725, 0.8627451 , 0.83495146, 0.84313725,\n",
       "        0.81188119, 0.8627451 , 0.85436893, 0.85436893, 0.82352941,\n",
       "        0.82352941, 0.84      , 0.83495146, 0.82692308, 0.81904762,\n",
       "        0.8627451 , 0.85436893, 0.8627451 , 0.83495146, 0.83495146,\n",
       "        0.84313725, 0.84313725, 0.84313725, 0.81553398, 0.85148515,\n",
       "        0.85436893, 0.84615385, 0.83495146, 0.84313725, 0.83495146,\n",
       "        0.84      , 0.85714286, 0.85714286, 0.85148515, 0.86538462,\n",
       "        0.85714286, 0.82352941, 0.87378641, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84313725, 0.85148515, 0.84615385, 0.83495146,\n",
       "        0.83495146, 0.85436893, 0.85436893, 0.82352941, 0.84313725,\n",
       "        0.81188119, 0.85148515, 0.84313725, 0.83168317, 0.82692308,\n",
       "        0.82692308, 0.84615385, 0.83168317, 0.81188119, 0.81553398,\n",
       "        0.84313725, 0.85436893, 0.85436893, 0.83495146, 0.84615385,\n",
       "        0.84313725, 0.84      , 0.85148515, 0.83809524, 0.84313725,\n",
       "        0.8627451 , 0.8627451 , 0.84615385, 0.86538462, 0.86      ,\n",
       "        0.83168317, 0.83809524, 0.86538462, 0.84      , 0.84615385,\n",
       "        0.85714286, 0.84313725, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.85436893, 0.85436893, 0.83673469, 0.8627451 , 0.85714286,\n",
       "        0.82352941, 0.83018868, 0.86538462, 0.81553398, 0.83809524,\n",
       "        0.82692308, 0.83168317, 0.85436893, 0.81553398, 0.84313725,\n",
       "        0.85714286, 0.84615385, 0.82      , 0.84313725, 0.85148515,\n",
       "        0.84      , 0.84      , 0.8627451 , 0.85436893, 0.85714286,\n",
       "        0.82692308, 0.83168317, 0.82      , 0.84313725, 0.83168317,\n",
       "        0.84      , 0.85436893, 0.83168317, 0.83495146, 0.83495146,\n",
       "        0.81632653, 0.84615385, 0.85436893, 0.83333333, 0.85436893,\n",
       "        0.84615385, 0.82828283, 0.84615385, 0.8627451 , 0.81632653,\n",
       "        0.83495146, 0.85714286, 0.81632653, 0.85148515, 0.85436893,\n",
       "        0.80808081, 0.83495146, 0.83809524, 0.81188119, 0.83495146,\n",
       "        0.85436893, 0.82      , 0.85148515, 0.84313725, 0.83168317,\n",
       "        0.83495146, 0.85714286, 0.82      , 0.8627451 , 0.8627451 ,\n",
       "        0.83168317, 0.85148515, 0.84313725, 0.83168317, 0.84615385,\n",
       "        0.8490566 , 0.82352941, 0.84313725, 0.81188119, 0.83168317,\n",
       "        0.85148515, 0.85148515, 0.84313725, 0.84313725, 0.8490566 ,\n",
       "        0.75862069, 0.82828283, 0.82      , 0.75      , 0.83673469,\n",
       "        0.84848485, 0.75555556, 0.82828283, 0.83168317, 0.79120879,\n",
       "        0.81632653, 0.80808081, 0.76923077, 0.82474227, 0.84848485,\n",
       "        0.76923077, 0.81188119, 0.83168317, 0.76086957, 0.82      ,\n",
       "        0.84      , 0.77419355, 0.82      , 0.84848485, 0.79166667,\n",
       "        0.82352941, 0.83495146, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.77894737, 0.83168317, 0.84313725, 0.77894737, 0.82352941,\n",
       "        0.84313725, 0.79166667, 0.81632653, 0.82828283, 0.79591837,\n",
       "        0.84313725, 0.84313725, 0.78350515, 0.83168317, 0.84313725,\n",
       "        0.83168317, 0.83809524, 0.84313725, 0.84313725, 0.85714286,\n",
       "        0.83809524, 0.85436893, 0.85714286, 0.87378641, 0.85148515,\n",
       "        0.85436893, 0.81188119, 0.85148515, 0.85714286, 0.83495146,\n",
       "        0.84313725, 0.84313725, 0.8627451 , 0.83495146, 0.84313725,\n",
       "        0.81188119, 0.8627451 , 0.85436893, 0.85436893, 0.82352941,\n",
       "        0.82352941, 0.84      , 0.83495146, 0.82692308, 0.81904762,\n",
       "        0.8627451 , 0.85436893, 0.8627451 , 0.83495146, 0.83495146,\n",
       "        0.84313725, 0.84313725, 0.84313725, 0.81553398, 0.85148515,\n",
       "        0.85436893, 0.84615385, 0.83495146, 0.84313725, 0.83495146,\n",
       "        0.84      , 0.85714286, 0.85714286, 0.85148515, 0.86538462,\n",
       "        0.85714286, 0.82352941, 0.87378641, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84313725, 0.85148515, 0.84615385, 0.83495146,\n",
       "        0.83495146, 0.85436893, 0.85436893, 0.82352941, 0.84313725,\n",
       "        0.81188119, 0.85148515, 0.84313725, 0.83168317, 0.82692308,\n",
       "        0.82692308, 0.84615385, 0.83168317, 0.81188119, 0.81553398,\n",
       "        0.84313725, 0.85436893, 0.85436893, 0.83495146, 0.84615385,\n",
       "        0.84313725, 0.84      , 0.85148515, 0.83809524, 0.84313725,\n",
       "        0.8627451 , 0.8627451 , 0.84615385, 0.86538462, 0.86      ,\n",
       "        0.83168317, 0.83809524, 0.86538462, 0.84      , 0.84615385,\n",
       "        0.85714286, 0.84313725, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.85436893, 0.85436893, 0.83673469, 0.8627451 , 0.85714286,\n",
       "        0.82352941, 0.83018868, 0.86538462, 0.81553398, 0.83809524,\n",
       "        0.82692308, 0.83168317, 0.85436893, 0.81553398, 0.84313725,\n",
       "        0.85714286, 0.84615385, 0.82      , 0.84313725, 0.85148515,\n",
       "        0.84      , 0.84      , 0.8627451 , 0.85436893, 0.85714286,\n",
       "        0.82692308, 0.83168317, 0.82      , 0.84313725, 0.83168317,\n",
       "        0.84      , 0.85436893, 0.83168317, 0.83495146, 0.83495146,\n",
       "        0.81632653, 0.84615385, 0.85436893, 0.83333333, 0.85436893,\n",
       "        0.84615385, 0.82828283, 0.84615385, 0.8627451 , 0.81632653,\n",
       "        0.83495146, 0.85714286, 0.81632653, 0.85148515, 0.85436893,\n",
       "        0.80808081, 0.83495146, 0.83809524, 0.81188119, 0.83495146,\n",
       "        0.85436893, 0.82      , 0.85148515, 0.84313725, 0.83168317,\n",
       "        0.83495146, 0.85714286, 0.82      , 0.8627451 , 0.8627451 ,\n",
       "        0.83168317, 0.85148515, 0.84313725, 0.83168317, 0.84615385,\n",
       "        0.8490566 , 0.82352941, 0.84313725, 0.81188119, 0.83168317,\n",
       "        0.85148515, 0.85148515, 0.84313725, 0.84313725, 0.8490566 ,\n",
       "        0.75862069, 0.82828283, 0.82      , 0.75      , 0.83673469,\n",
       "        0.84848485, 0.75555556, 0.82828283, 0.83168317, 0.79120879,\n",
       "        0.81632653, 0.80808081, 0.76923077, 0.82474227, 0.84848485,\n",
       "        0.76923077, 0.81188119, 0.83168317, 0.76086957, 0.82      ,\n",
       "        0.84      , 0.77419355, 0.82      , 0.84848485, 0.79166667,\n",
       "        0.82352941, 0.83495146, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.77894737, 0.83168317, 0.84313725, 0.77894737, 0.82352941,\n",
       "        0.84313725, 0.79166667, 0.81632653, 0.82828283, 0.79591837,\n",
       "        0.84313725, 0.84313725, 0.78350515, 0.83168317, 0.84313725,\n",
       "        0.84313725, 0.84615385, 0.83809524, 0.85436893, 0.86538462,\n",
       "        0.86538462, 0.84615385, 0.86538462, 0.87378641, 0.84      ,\n",
       "        0.83809524, 0.82692308, 0.86      , 0.86538462, 0.85436893,\n",
       "        0.85148515, 0.85436893, 0.85436893, 0.85148515, 0.85148515,\n",
       "        0.82692308, 0.85148515, 0.83495146, 0.82692308, 0.86868687,\n",
       "        0.84615385, 0.82692308, 0.82692308, 0.83495146, 0.78095238,\n",
       "        0.84313725, 0.84615385, 0.84615385, 0.85148515, 0.83495146,\n",
       "        0.84313725, 0.83168317, 0.82      , 0.78846154, 0.8627451 ,\n",
       "        0.84615385, 0.84615385, 0.84313725, 0.84615385, 0.84313725,\n",
       "        0.85148515, 0.85436893, 0.85436893, 0.85148515, 0.84615385,\n",
       "        0.8490566 , 0.85436893, 0.87378641, 0.86538462, 0.84      ,\n",
       "        0.85436893, 0.84615385, 0.84848485, 0.88235294, 0.85714286,\n",
       "        0.86      , 0.8627451 , 0.85436893, 0.86      , 0.82692308,\n",
       "        0.82692308, 0.86      , 0.84313725, 0.83495146, 0.86      ,\n",
       "        0.8627451 , 0.84313725, 0.82      , 0.83495146, 0.84313725,\n",
       "        0.84848485, 0.84313725, 0.84313725, 0.86      , 0.85714286,\n",
       "        0.84615385, 0.84      , 0.83495146, 0.80769231, 0.84848485,\n",
       "        0.85436893, 0.8627451 , 0.84313725, 0.8490566 , 0.84615385,\n",
       "        0.84536082, 0.84      , 0.87378641, 0.84536082, 0.8627451 ,\n",
       "        0.87378641, 0.84848485, 0.8627451 , 0.87378641, 0.84      ,\n",
       "        0.85148515, 0.85148515, 0.85714286, 0.87128713, 0.84615385,\n",
       "        0.84      , 0.84615385, 0.85436893, 0.83168317, 0.85436893,\n",
       "        0.85436893, 0.83673469, 0.85148515, 0.84313725, 0.84313725,\n",
       "        0.84615385, 0.85714286, 0.82      , 0.84313725, 0.85148515,\n",
       "        0.83673469, 0.86538462, 0.84615385, 0.8627451 , 0.85436893,\n",
       "        0.85714286, 0.84848485, 0.84313725, 0.82352941, 0.84848485,\n",
       "        0.85148515, 0.84615385, 0.83168317, 0.8490566 , 0.85714286,\n",
       "        0.82978723, 0.84313725, 0.83168317, 0.83870968, 0.85148515,\n",
       "        0.8627451 , 0.82828283, 0.85436893, 0.85436893, 0.8125    ,\n",
       "        0.84      , 0.85436893, 0.80851064, 0.85148515, 0.8627451 ,\n",
       "        0.8       , 0.85148515, 0.85148515, 0.77894737, 0.86      ,\n",
       "        0.8627451 , 0.78723404, 0.85148515, 0.8627451 , 0.79166667,\n",
       "        0.8627451 , 0.85436893, 0.80412371, 0.83168317, 0.85436893,\n",
       "        0.8       , 0.85148515, 0.85148515, 0.81632653, 0.84615385,\n",
       "        0.84615385, 0.82474227, 0.84      , 0.84313725, 0.83333333,\n",
       "        0.84313725, 0.86538462, 0.81632653, 0.84615385, 0.84615385,\n",
       "        0.77272727, 0.84210526, 0.81188119, 0.77272727, 0.84210526,\n",
       "        0.83168317, 0.75      , 0.83333333, 0.84615385, 0.75      ,\n",
       "        0.8125    , 0.82      , 0.75      , 0.82105263, 0.83673469,\n",
       "        0.75      , 0.79591837, 0.84      , 0.75555556, 0.81632653,\n",
       "        0.83168317, 0.75      , 0.8125    , 0.84      , 0.74157303,\n",
       "        0.78787879, 0.83495146, 0.74725275, 0.82828283, 0.83168317,\n",
       "        0.75      , 0.8       , 0.84      , 0.74725275, 0.80808081,\n",
       "        0.83495146, 0.75555556, 0.81632653, 0.82      , 0.74157303,\n",
       "        0.83673469, 0.84      , 0.74725275, 0.80808081, 0.83495146]),\n",
       " 'split1_test_score': array([0.72727273, 0.71578947, 0.68686869, 0.70833333, 0.71428571,\n",
       "        0.71428571, 0.70103093, 0.70833333, 0.72164948, 0.69473684,\n",
       "        0.7       , 0.68627451, 0.68085106, 0.70833333, 0.72      ,\n",
       "        0.69473684, 0.72916667, 0.74747475, 0.70833333, 0.72      ,\n",
       "        0.69306931, 0.70103093, 0.74747475, 0.74      , 0.71428571,\n",
       "        0.70833333, 0.74      , 0.72164948, 0.7254902 , 0.69902913,\n",
       "        0.70103093, 0.74      , 0.7254902 , 0.71428571, 0.71428571,\n",
       "        0.74747475, 0.73469388, 0.72727273, 0.7184466 , 0.72727273,\n",
       "        0.74747475, 0.74747475, 0.70833333, 0.71428571, 0.73267327,\n",
       "        0.68085106, 0.71428571, 0.70103093, 0.6875    , 0.73469388,\n",
       "        0.70833333, 0.69473684, 0.70833333, 0.70103093, 0.6875    ,\n",
       "        0.72164948, 0.70103093, 0.70103093, 0.72164948, 0.71578947,\n",
       "        0.70103093, 0.70833333, 0.72164948, 0.70103093, 0.69387755,\n",
       "        0.7       , 0.71428571, 0.72727273, 0.74      , 0.69473684,\n",
       "        0.70833333, 0.71578947, 0.70103093, 0.7254902 , 0.72      ,\n",
       "        0.70103093, 0.71428571, 0.74      , 0.70103093, 0.70103093,\n",
       "        0.74      , 0.72727273, 0.72      , 0.70588235, 0.70833333,\n",
       "        0.73469388, 0.76      , 0.6875    , 0.72916667, 0.74747475,\n",
       "        0.68041237, 0.6875    , 0.70103093, 0.6875    , 0.72727273,\n",
       "        0.73469388, 0.68041237, 0.70833333, 0.70833333, 0.69387755,\n",
       "        0.71578947, 0.71428571, 0.70103093, 0.72916667, 0.74226804,\n",
       "        0.70103093, 0.72164948, 0.72916667, 0.68041237, 0.73469388,\n",
       "        0.7       , 0.71428571, 0.74226804, 0.72916667, 0.6875    ,\n",
       "        0.70103093, 0.70833333, 0.68041237, 0.71428571, 0.71287129,\n",
       "        0.71428571, 0.73469388, 0.73469388, 0.70103093, 0.70103093,\n",
       "        0.71578947, 0.68041237, 0.72727273, 0.73267327, 0.71428571,\n",
       "        0.72164948, 0.73469388, 0.68041237, 0.70103093, 0.70833333,\n",
       "        0.66666667, 0.70103093, 0.71428571, 0.68041237, 0.70103093,\n",
       "        0.70103093, 0.67346939, 0.70103093, 0.70833333, 0.65263158,\n",
       "        0.70103093, 0.71428571, 0.66666667, 0.71578947, 0.71578947,\n",
       "        0.68041237, 0.70833333, 0.70833333, 0.68041237, 0.70707071,\n",
       "        0.70103093, 0.66666667, 0.70103093, 0.72916667, 0.67346939,\n",
       "        0.70833333, 0.71428571, 0.67346939, 0.70833333, 0.70103093,\n",
       "        0.66666667, 0.70103093, 0.71428571, 0.68041237, 0.72164948,\n",
       "        0.70103093, 0.67346939, 0.72164948, 0.73469388, 0.68041237,\n",
       "        0.6875    , 0.73469388, 0.68041237, 0.71428571, 0.70103093,\n",
       "        0.61052632, 0.66666667, 0.69387755, 0.60869565, 0.67368421,\n",
       "        0.72164948, 0.61538462, 0.67368421, 0.6875    , 0.64583333,\n",
       "        0.67368421, 0.6875    , 0.63917526, 0.66666667, 0.70833333,\n",
       "        0.63157895, 0.68041237, 0.70103093, 0.67346939, 0.67346939,\n",
       "        0.69387755, 0.65263158, 0.69387755, 0.6875    , 0.64583333,\n",
       "        0.69387755, 0.70833333, 0.65979381, 0.68041237, 0.69387755,\n",
       "        0.65263158, 0.68041237, 0.6875    , 0.65263158, 0.69387755,\n",
       "        0.70833333, 0.68686869, 0.69387755, 0.70103093, 0.65979381,\n",
       "        0.67368421, 0.67368421, 0.65263158, 0.70103093, 0.70833333,\n",
       "        0.72727273, 0.71578947, 0.68686869, 0.70833333, 0.71428571,\n",
       "        0.71428571, 0.70103093, 0.70833333, 0.72164948, 0.69473684,\n",
       "        0.7       , 0.68627451, 0.68085106, 0.70833333, 0.72      ,\n",
       "        0.69473684, 0.72916667, 0.74747475, 0.70833333, 0.72      ,\n",
       "        0.69306931, 0.70103093, 0.74747475, 0.74      , 0.71428571,\n",
       "        0.70833333, 0.74      , 0.72164948, 0.7254902 , 0.69902913,\n",
       "        0.70103093, 0.74      , 0.7254902 , 0.71428571, 0.71428571,\n",
       "        0.74747475, 0.73469388, 0.72727273, 0.7184466 , 0.72727273,\n",
       "        0.74747475, 0.74747475, 0.70833333, 0.71428571, 0.73267327,\n",
       "        0.68085106, 0.71428571, 0.70103093, 0.6875    , 0.73469388,\n",
       "        0.70833333, 0.69473684, 0.70833333, 0.70103093, 0.6875    ,\n",
       "        0.72164948, 0.70103093, 0.70103093, 0.72164948, 0.71578947,\n",
       "        0.70103093, 0.70833333, 0.72164948, 0.70103093, 0.69387755,\n",
       "        0.7       , 0.71428571, 0.72727273, 0.74      , 0.69473684,\n",
       "        0.70833333, 0.71578947, 0.70103093, 0.7254902 , 0.72      ,\n",
       "        0.70103093, 0.71428571, 0.74      , 0.70103093, 0.70103093,\n",
       "        0.74      , 0.72727273, 0.72      , 0.70588235, 0.70833333,\n",
       "        0.73469388, 0.76      , 0.6875    , 0.72916667, 0.74747475,\n",
       "        0.68041237, 0.6875    , 0.70103093, 0.6875    , 0.72727273,\n",
       "        0.73469388, 0.68041237, 0.70833333, 0.70833333, 0.69387755,\n",
       "        0.71578947, 0.71428571, 0.70103093, 0.72916667, 0.74226804,\n",
       "        0.70103093, 0.72164948, 0.72916667, 0.68041237, 0.73469388,\n",
       "        0.7       , 0.71428571, 0.74226804, 0.72916667, 0.6875    ,\n",
       "        0.70103093, 0.70833333, 0.68041237, 0.71428571, 0.71287129,\n",
       "        0.71428571, 0.73469388, 0.73469388, 0.70103093, 0.70103093,\n",
       "        0.71578947, 0.68041237, 0.72727273, 0.73267327, 0.71428571,\n",
       "        0.72164948, 0.73469388, 0.68041237, 0.70103093, 0.70833333,\n",
       "        0.66666667, 0.70103093, 0.71428571, 0.68041237, 0.70103093,\n",
       "        0.70103093, 0.67346939, 0.70103093, 0.70833333, 0.65263158,\n",
       "        0.70103093, 0.71428571, 0.66666667, 0.71578947, 0.71578947,\n",
       "        0.68041237, 0.70833333, 0.70833333, 0.68041237, 0.70707071,\n",
       "        0.70103093, 0.66666667, 0.70103093, 0.72916667, 0.67346939,\n",
       "        0.70833333, 0.71428571, 0.67346939, 0.70833333, 0.70103093,\n",
       "        0.66666667, 0.70103093, 0.71428571, 0.68041237, 0.72164948,\n",
       "        0.70103093, 0.67346939, 0.72164948, 0.73469388, 0.68041237,\n",
       "        0.6875    , 0.73469388, 0.68041237, 0.71428571, 0.70103093,\n",
       "        0.61052632, 0.66666667, 0.69387755, 0.60869565, 0.67368421,\n",
       "        0.72164948, 0.61538462, 0.67368421, 0.6875    , 0.64583333,\n",
       "        0.67368421, 0.6875    , 0.63917526, 0.66666667, 0.70833333,\n",
       "        0.63157895, 0.68041237, 0.70103093, 0.67346939, 0.67346939,\n",
       "        0.69387755, 0.65263158, 0.69387755, 0.6875    , 0.64583333,\n",
       "        0.69387755, 0.70833333, 0.65979381, 0.68041237, 0.69387755,\n",
       "        0.65263158, 0.68041237, 0.6875    , 0.65263158, 0.69387755,\n",
       "        0.70833333, 0.68686869, 0.69387755, 0.70103093, 0.65979381,\n",
       "        0.67368421, 0.67368421, 0.65263158, 0.70103093, 0.70833333,\n",
       "        0.70707071, 0.73469388, 0.71428571, 0.70103093, 0.70833333,\n",
       "        0.72      , 0.70103093, 0.70103093, 0.70103093, 0.6875    ,\n",
       "        0.70103093, 0.6875    , 0.70103093, 0.71428571, 0.70707071,\n",
       "        0.70103093, 0.69473684, 0.70833333, 0.70103093, 0.73267327,\n",
       "        0.70588235, 0.70103093, 0.66666667, 0.71287129, 0.69387755,\n",
       "        0.69387755, 0.72      , 0.70707071, 0.71287129, 0.69306931,\n",
       "        0.70103093, 0.72727273, 0.73267327, 0.70103093, 0.70103093,\n",
       "        0.74      , 0.68686869, 0.73267327, 0.70707071, 0.70103093,\n",
       "        0.7       , 0.7184466 , 0.69387755, 0.70707071, 0.74      ,\n",
       "        0.68041237, 0.71428571, 0.72727273, 0.6875    , 0.70103093,\n",
       "        0.70833333, 0.68041237, 0.70707071, 0.72916667, 0.69387755,\n",
       "        0.72916667, 0.70707071, 0.70103093, 0.70103093, 0.72916667,\n",
       "        0.70103093, 0.70833333, 0.70833333, 0.68041237, 0.72164948,\n",
       "        0.7254902 , 0.70103093, 0.71428571, 0.71428571, 0.70103093,\n",
       "        0.69473684, 0.72164948, 0.67346939, 0.73469388, 0.72      ,\n",
       "        0.70103093, 0.71428571, 0.69387755, 0.69387755, 0.71428571,\n",
       "        0.72727273, 0.7       , 0.74      , 0.72      , 0.70833333,\n",
       "        0.72164948, 0.73267327, 0.69387755, 0.72164948, 0.72164948,\n",
       "        0.67346939, 0.71428571, 0.72727273, 0.67346939, 0.71428571,\n",
       "        0.72164948, 0.6875    , 0.70103093, 0.70833333, 0.68686869,\n",
       "        0.70833333, 0.71428571, 0.68041237, 0.70103093, 0.72727273,\n",
       "        0.70103093, 0.70103093, 0.70833333, 0.67346939, 0.70833333,\n",
       "        0.70103093, 0.70103093, 0.71428571, 0.70103093, 0.70103093,\n",
       "        0.70103093, 0.70833333, 0.65979381, 0.72164948, 0.71428571,\n",
       "        0.67346939, 0.70833333, 0.69473684, 0.69387755, 0.70103093,\n",
       "        0.70103093, 0.65979381, 0.72164948, 0.7184466 , 0.67346939,\n",
       "        0.72164948, 0.6875    , 0.68041237, 0.70103093, 0.70833333,\n",
       "        0.67346939, 0.72727273, 0.71428571, 0.66      , 0.70103093,\n",
       "        0.70103093, 0.66      , 0.70103093, 0.70103093, 0.63917526,\n",
       "        0.68041237, 0.69473684, 0.65306122, 0.70103093, 0.70103093,\n",
       "        0.65979381, 0.70103093, 0.70103093, 0.65979381, 0.68041237,\n",
       "        0.71428571, 0.65979381, 0.70833333, 0.70833333, 0.68041237,\n",
       "        0.70103093, 0.70103093, 0.65979381, 0.68041237, 0.71428571,\n",
       "        0.67346939, 0.72164948, 0.70833333, 0.68041237, 0.70103093,\n",
       "        0.6875    , 0.65306122, 0.68041237, 0.72916667, 0.67346939,\n",
       "        0.72164948, 0.72164948, 0.67346939, 0.70103093, 0.70103093,\n",
       "        0.61538462, 0.65306122, 0.65979381, 0.59340659, 0.65306122,\n",
       "        0.6875    , 0.59340659, 0.66666667, 0.6875    , 0.60215054,\n",
       "        0.63917526, 0.65979381, 0.57777778, 0.64583333, 0.66666667,\n",
       "        0.59340659, 0.65979381, 0.70103093, 0.61702128, 0.65306122,\n",
       "        0.66666667, 0.61702128, 0.65306122, 0.68686869, 0.61702128,\n",
       "        0.66666667, 0.70103093, 0.60215054, 0.65306122, 0.66666667,\n",
       "        0.61702128, 0.65306122, 0.68041237, 0.62365591, 0.67346939,\n",
       "        0.70103093, 0.61702128, 0.65306122, 0.66666667, 0.61702128,\n",
       "        0.63917526, 0.67346939, 0.63157895, 0.67346939, 0.70103093]),\n",
       " 'split2_test_score': array([0.75471698, 0.76923077, 0.74074074, 0.75      , 0.75      ,\n",
       "        0.73584906, 0.75728155, 0.77669903, 0.76190476, 0.74285714,\n",
       "        0.75      , 0.72897196, 0.76190476, 0.73076923, 0.75471698,\n",
       "        0.75      , 0.75471698, 0.75471698, 0.76190476, 0.75471698,\n",
       "        0.75471698, 0.76923077, 0.74766355, 0.74766355, 0.75728155,\n",
       "        0.76923077, 0.74074074, 0.73584906, 0.74074074, 0.72727273,\n",
       "        0.75      , 0.75471698, 0.74074074, 0.75      , 0.74074074,\n",
       "        0.73394495, 0.75471698, 0.73394495, 0.71559633, 0.75728155,\n",
       "        0.75471698, 0.73394495, 0.75      , 0.75471698, 0.72072072,\n",
       "        0.78431373, 0.74285714, 0.76923077, 0.75728155, 0.73786408,\n",
       "        0.75      , 0.75728155, 0.74509804, 0.75728155, 0.76923077,\n",
       "        0.73584906, 0.74285714, 0.76923077, 0.74509804, 0.75      ,\n",
       "        0.75728155, 0.76470588, 0.76190476, 0.75      , 0.75471698,\n",
       "        0.74285714, 0.75      , 0.75471698, 0.75471698, 0.75      ,\n",
       "        0.76470588, 0.75      , 0.75      , 0.74766355, 0.74766355,\n",
       "        0.75      , 0.76190476, 0.75471698, 0.75      , 0.75728155,\n",
       "        0.73394495, 0.75      , 0.73394495, 0.74074074, 0.75      ,\n",
       "        0.74766355, 0.74766355, 0.75      , 0.76190476, 0.72727273,\n",
       "        0.77227723, 0.75      , 0.75471698, 0.76      , 0.74509804,\n",
       "        0.75728155, 0.77227723, 0.75728155, 0.76923077, 0.76470588,\n",
       "        0.76923077, 0.73584906, 0.76470588, 0.75      , 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.76190476, 0.75728155, 0.75471698,\n",
       "        0.73584906, 0.75728155, 0.73786408, 0.74285714, 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.74285714, 0.74285714, 0.74074074,\n",
       "        0.75      , 0.75      , 0.74766355, 0.75      , 0.75      ,\n",
       "        0.76190476, 0.74285714, 0.74766355, 0.74766355, 0.75      ,\n",
       "        0.74285714, 0.75471698, 0.75      , 0.75471698, 0.76923077,\n",
       "        0.77227723, 0.77669903, 0.73076923, 0.76      , 0.76470588,\n",
       "        0.73786408, 0.77227723, 0.76470588, 0.75728155, 0.77227723,\n",
       "        0.75728155, 0.75728155, 0.77227723, 0.75728155, 0.74509804,\n",
       "        0.75      , 0.75728155, 0.75728155, 0.75      , 0.75      ,\n",
       "        0.76190476, 0.75      , 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.75728155, 0.75728155, 0.73076923, 0.73786408, 0.73584906,\n",
       "        0.75      , 0.75      , 0.73786408, 0.74285714, 0.74285714,\n",
       "        0.74285714, 0.75      , 0.73584906, 0.74074074, 0.74285714,\n",
       "        0.74285714, 0.76190476, 0.73584906, 0.75      , 0.75471698,\n",
       "        0.76470588, 0.74747475, 0.74747475, 0.76470588, 0.74747475,\n",
       "        0.76      , 0.76470588, 0.76      , 0.77227723, 0.75247525,\n",
       "        0.77227723, 0.77227723, 0.76470588, 0.76      , 0.77227723,\n",
       "        0.75728155, 0.75728155, 0.75728155, 0.76470588, 0.76470588,\n",
       "        0.74509804, 0.78431373, 0.73786408, 0.74509804, 0.74285714,\n",
       "        0.75728155, 0.75728155, 0.75247525, 0.75728155, 0.74509804,\n",
       "        0.77227723, 0.75247525, 0.73786408, 0.72897196, 0.75      ,\n",
       "        0.75      , 0.74509804, 0.75      , 0.72380952, 0.76470588,\n",
       "        0.73786408, 0.73786408, 0.73584906, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.76923077, 0.74074074, 0.75      , 0.75      ,\n",
       "        0.73584906, 0.75728155, 0.77669903, 0.76190476, 0.74285714,\n",
       "        0.75      , 0.72897196, 0.76190476, 0.73076923, 0.75471698,\n",
       "        0.75      , 0.75471698, 0.75471698, 0.76190476, 0.75471698,\n",
       "        0.75471698, 0.76923077, 0.74766355, 0.74766355, 0.75728155,\n",
       "        0.76923077, 0.74074074, 0.73584906, 0.74074074, 0.72727273,\n",
       "        0.75      , 0.75471698, 0.74074074, 0.75      , 0.74074074,\n",
       "        0.73394495, 0.75471698, 0.73394495, 0.71559633, 0.75728155,\n",
       "        0.75471698, 0.73394495, 0.75      , 0.75471698, 0.72072072,\n",
       "        0.78431373, 0.74285714, 0.76923077, 0.75728155, 0.73786408,\n",
       "        0.75      , 0.75728155, 0.74509804, 0.75728155, 0.76923077,\n",
       "        0.73584906, 0.74285714, 0.76923077, 0.74509804, 0.75      ,\n",
       "        0.75728155, 0.76470588, 0.76190476, 0.75      , 0.75471698,\n",
       "        0.74285714, 0.75      , 0.75471698, 0.75471698, 0.75      ,\n",
       "        0.76470588, 0.75      , 0.75      , 0.74766355, 0.74766355,\n",
       "        0.75      , 0.76190476, 0.75471698, 0.75      , 0.75728155,\n",
       "        0.73394495, 0.75      , 0.73394495, 0.74074074, 0.75      ,\n",
       "        0.74766355, 0.74766355, 0.75      , 0.76190476, 0.72727273,\n",
       "        0.77227723, 0.75      , 0.75471698, 0.76      , 0.74509804,\n",
       "        0.75728155, 0.77227723, 0.75728155, 0.76923077, 0.76470588,\n",
       "        0.76923077, 0.73584906, 0.76470588, 0.75      , 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.76190476, 0.75728155, 0.75471698,\n",
       "        0.73584906, 0.75728155, 0.73786408, 0.74285714, 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.74285714, 0.74285714, 0.74074074,\n",
       "        0.75      , 0.75      , 0.74766355, 0.75      , 0.75      ,\n",
       "        0.76190476, 0.74285714, 0.74766355, 0.74766355, 0.75      ,\n",
       "        0.74285714, 0.75471698, 0.75      , 0.75471698, 0.76923077,\n",
       "        0.77227723, 0.77669903, 0.73076923, 0.76      , 0.76470588,\n",
       "        0.73786408, 0.77227723, 0.76470588, 0.75728155, 0.77227723,\n",
       "        0.75728155, 0.75728155, 0.77227723, 0.75728155, 0.74509804,\n",
       "        0.75      , 0.75728155, 0.75728155, 0.75      , 0.75      ,\n",
       "        0.76190476, 0.75      , 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.75728155, 0.75728155, 0.73076923, 0.73786408, 0.73584906,\n",
       "        0.75      , 0.75      , 0.73786408, 0.74285714, 0.74285714,\n",
       "        0.74285714, 0.75      , 0.73584906, 0.74074074, 0.74285714,\n",
       "        0.74285714, 0.76190476, 0.73584906, 0.75      , 0.75471698,\n",
       "        0.76470588, 0.74747475, 0.74747475, 0.76470588, 0.74747475,\n",
       "        0.76      , 0.76470588, 0.76      , 0.77227723, 0.75247525,\n",
       "        0.77227723, 0.77227723, 0.76470588, 0.76      , 0.77227723,\n",
       "        0.75728155, 0.75728155, 0.75728155, 0.76470588, 0.76470588,\n",
       "        0.74509804, 0.78431373, 0.73786408, 0.74509804, 0.74285714,\n",
       "        0.75728155, 0.75728155, 0.75247525, 0.75728155, 0.74509804,\n",
       "        0.77227723, 0.75247525, 0.73786408, 0.72897196, 0.75      ,\n",
       "        0.75      , 0.74509804, 0.75      , 0.72380952, 0.76470588,\n",
       "        0.73786408, 0.73786408, 0.73584906, 0.75      , 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.75      , 0.76190476,\n",
       "        0.74285714, 0.75728155, 0.76923077, 0.76923077, 0.73076923,\n",
       "        0.72897196, 0.74766355, 0.75      , 0.76923077, 0.75471698,\n",
       "        0.75      , 0.76923077, 0.76190476, 0.73076923, 0.72897196,\n",
       "        0.73394495, 0.75      , 0.75471698, 0.75471698, 0.75728155,\n",
       "        0.76923077, 0.76190476, 0.72380952, 0.71028037, 0.71559633,\n",
       "        0.75      , 0.74074074, 0.73394495, 0.75      , 0.76923077,\n",
       "        0.74766355, 0.73584906, 0.74074074, 0.72727273, 0.75      ,\n",
       "        0.75471698, 0.74074074, 0.75      , 0.75471698, 0.74766355,\n",
       "        0.77227723, 0.77358491, 0.76190476, 0.76470588, 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.76923077, 0.75      , 0.76470588,\n",
       "        0.75      , 0.74285714, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75728155, 0.76923077, 0.76190476, 0.74285714, 0.74074074,\n",
       "        0.72897196, 0.75728155, 0.76923077, 0.74766355, 0.76470588,\n",
       "        0.77669903, 0.76923077, 0.73076923, 0.73584906, 0.72222222,\n",
       "        0.75      , 0.73584906, 0.74074074, 0.75      , 0.76923077,\n",
       "        0.76923077, 0.72380952, 0.72897196, 0.72222222, 0.75      ,\n",
       "        0.74766355, 0.74074074, 0.75728155, 0.76190476, 0.76190476,\n",
       "        0.76      , 0.76923077, 0.75471698, 0.77227723, 0.78095238,\n",
       "        0.75728155, 0.76470588, 0.76923077, 0.75      , 0.76470588,\n",
       "        0.75      , 0.74285714, 0.77227723, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.76923077, 0.77669903, 0.75      , 0.75471698,\n",
       "        0.73584906, 0.75      , 0.76923077, 0.75471698, 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.74285714, 0.74285714, 0.72897196,\n",
       "        0.74285714, 0.75      , 0.74766355, 0.75728155, 0.76923077,\n",
       "        0.76190476, 0.74285714, 0.75471698, 0.73394495, 0.75      ,\n",
       "        0.75471698, 0.74766355, 0.75728155, 0.76923077, 0.76923077,\n",
       "        0.77227723, 0.77227723, 0.75      , 0.77227723, 0.77227723,\n",
       "        0.75728155, 0.77227723, 0.75728155, 0.76923077, 0.74509804,\n",
       "        0.75247525, 0.75      , 0.75728155, 0.76923077, 0.76923077,\n",
       "        0.75728155, 0.76470588, 0.76923077, 0.74509804, 0.75728155,\n",
       "        0.75471698, 0.75728155, 0.75728155, 0.75728155, 0.75      ,\n",
       "        0.75728155, 0.76923077, 0.74509804, 0.73076923, 0.75      ,\n",
       "        0.75728155, 0.75      , 0.75728155, 0.75728155, 0.75      ,\n",
       "        0.76923077, 0.74509804, 0.73076923, 0.74285714, 0.76470588,\n",
       "        0.75      , 0.75728155, 0.76470588, 0.75      , 0.76190476,\n",
       "        0.72916667, 0.75247525, 0.76      , 0.74468085, 0.75247525,\n",
       "        0.76      , 0.74468085, 0.76      , 0.77227723, 0.72164948,\n",
       "        0.75247525, 0.75247525, 0.73684211, 0.74      , 0.76470588,\n",
       "        0.7628866 , 0.75247525, 0.76470588, 0.77227723, 0.75247525,\n",
       "        0.75247525, 0.76      , 0.76470588, 0.76470588, 0.75247525,\n",
       "        0.75728155, 0.75      , 0.76767677, 0.76767677, 0.74509804,\n",
       "        0.76767677, 0.75728155, 0.75      , 0.76470588, 0.75      ,\n",
       "        0.75728155, 0.76      , 0.75247525, 0.74509804, 0.74747475,\n",
       "        0.76470588, 0.75      , 0.77227723, 0.75728155, 0.75728155]),\n",
       " 'split3_test_score': array([0.7961165 , 0.78095238, 0.77358491, 0.78095238, 0.78095238,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.8       ,\n",
       "        0.78095238, 0.76190476, 0.79245283, 0.78095238, 0.78095238,\n",
       "        0.81132075, 0.7961165 , 0.8       , 0.79245283, 0.76190476,\n",
       "        0.77358491, 0.78846154, 0.76190476, 0.79245283, 0.80769231,\n",
       "        0.7961165 , 0.78846154, 0.78504673, 0.76190476, 0.76190476,\n",
       "        0.8       , 0.76923077, 0.76923077, 0.81904762, 0.80769231,\n",
       "        0.78846154, 0.78095238, 0.76923077, 0.76923077, 0.7962963 ,\n",
       "        0.76190476, 0.77358491, 0.80769231, 0.8       , 0.78504673,\n",
       "        0.78431373, 0.78095238, 0.76190476, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.8       , 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.77358491, 0.8       , 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.79245283, 0.76190476,\n",
       "        0.76190476, 0.79245283, 0.78095238, 0.75      , 0.78095238,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.76923077, 0.76923077,\n",
       "        0.80373832, 0.77358491, 0.78095238, 0.81904762, 0.7961165 ,\n",
       "        0.78095238, 0.76923077, 0.76923077, 0.76923077, 0.80769231,\n",
       "        0.75      , 0.78095238, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.76923077, 0.78095238,\n",
       "        0.78095238, 0.76923077, 0.8       , 0.8       , 0.78846154,\n",
       "        0.8       , 0.77358491, 0.78095238, 0.78846154, 0.77669903,\n",
       "        0.78095238, 0.80769231, 0.7961165 , 0.78095238, 0.76923077,\n",
       "        0.76923077, 0.78095238, 0.77669903, 0.77358491, 0.78095238,\n",
       "        0.80769231, 0.7961165 , 0.78846154, 0.76190476, 0.76923077,\n",
       "        0.79245283, 0.76190476, 0.76923077, 0.81132075, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76923077, 0.8       ,\n",
       "        0.77358491, 0.76923077, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.75728155, 0.77669903, 0.78095238, 0.75728155, 0.78846154,\n",
       "        0.78095238, 0.75728155, 0.78846154, 0.78504673, 0.76470588,\n",
       "        0.79245283, 0.78095238, 0.76470588, 0.79245283, 0.78846154,\n",
       "        0.75728155, 0.7961165 , 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.78846154, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.7961165 , 0.80769231, 0.78846154, 0.77669903, 0.76190476,\n",
       "        0.8       , 0.8       , 0.77669903, 0.77669903, 0.80769231,\n",
       "        0.7961165 , 0.79245283, 0.77669903, 0.76923077, 0.8       ,\n",
       "        0.80769231, 0.78095238, 0.77669903, 0.80769231, 0.7961165 ,\n",
       "        0.7628866 , 0.75728155, 0.76923077, 0.7628866 , 0.75728155,\n",
       "        0.76190476, 0.74      , 0.75728155, 0.76190476, 0.78350515,\n",
       "        0.75728155, 0.78846154, 0.78350515, 0.75728155, 0.76923077,\n",
       "        0.71428571, 0.75728155, 0.76923077, 0.78787879, 0.78095238,\n",
       "        0.78846154, 0.7755102 , 0.78095238, 0.78095238, 0.72727273,\n",
       "        0.76923077, 0.78095238, 0.78787879, 0.77669903, 0.80769231,\n",
       "        0.78787879, 0.77669903, 0.8       , 0.74509804, 0.78095238,\n",
       "        0.78846154, 0.80808081, 0.8       , 0.80769231, 0.78      ,\n",
       "        0.77669903, 0.8       , 0.75247525, 0.78846154, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.77358491, 0.78095238, 0.78095238,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.8       ,\n",
       "        0.78095238, 0.76190476, 0.79245283, 0.78095238, 0.78095238,\n",
       "        0.81132075, 0.7961165 , 0.8       , 0.79245283, 0.76190476,\n",
       "        0.77358491, 0.78846154, 0.76190476, 0.79245283, 0.80769231,\n",
       "        0.7961165 , 0.78846154, 0.78504673, 0.76190476, 0.76190476,\n",
       "        0.8       , 0.76923077, 0.76923077, 0.81904762, 0.80769231,\n",
       "        0.78846154, 0.78095238, 0.76923077, 0.76923077, 0.7962963 ,\n",
       "        0.76190476, 0.77358491, 0.80769231, 0.8       , 0.78504673,\n",
       "        0.78431373, 0.78095238, 0.76190476, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.8       , 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.77358491, 0.8       , 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.79245283, 0.76190476,\n",
       "        0.76190476, 0.79245283, 0.78095238, 0.75      , 0.78095238,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.76923077, 0.76923077,\n",
       "        0.80373832, 0.77358491, 0.78095238, 0.81904762, 0.7961165 ,\n",
       "        0.78095238, 0.76923077, 0.76923077, 0.76923077, 0.80769231,\n",
       "        0.75      , 0.78095238, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.76923077, 0.78095238,\n",
       "        0.78095238, 0.76923077, 0.8       , 0.8       , 0.78846154,\n",
       "        0.8       , 0.77358491, 0.78095238, 0.78846154, 0.77669903,\n",
       "        0.78095238, 0.80769231, 0.7961165 , 0.78095238, 0.76923077,\n",
       "        0.76923077, 0.78095238, 0.77669903, 0.77358491, 0.78095238,\n",
       "        0.80769231, 0.7961165 , 0.78846154, 0.76190476, 0.76923077,\n",
       "        0.79245283, 0.76190476, 0.76923077, 0.81132075, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76923077, 0.8       ,\n",
       "        0.77358491, 0.76923077, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.75728155, 0.77669903, 0.78095238, 0.75728155, 0.78846154,\n",
       "        0.78095238, 0.75728155, 0.78846154, 0.78504673, 0.76470588,\n",
       "        0.79245283, 0.78095238, 0.76470588, 0.79245283, 0.78846154,\n",
       "        0.75728155, 0.7961165 , 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.78846154, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.7961165 , 0.80769231, 0.78846154, 0.77669903, 0.76190476,\n",
       "        0.8       , 0.8       , 0.77669903, 0.77669903, 0.80769231,\n",
       "        0.7961165 , 0.79245283, 0.77669903, 0.76923077, 0.8       ,\n",
       "        0.80769231, 0.78095238, 0.77669903, 0.80769231, 0.7961165 ,\n",
       "        0.7628866 , 0.75728155, 0.76923077, 0.7628866 , 0.75728155,\n",
       "        0.76190476, 0.74      , 0.75728155, 0.76190476, 0.78350515,\n",
       "        0.75728155, 0.78846154, 0.78350515, 0.75728155, 0.76923077,\n",
       "        0.71428571, 0.75728155, 0.76923077, 0.78787879, 0.78095238,\n",
       "        0.78846154, 0.7755102 , 0.78095238, 0.78095238, 0.72727273,\n",
       "        0.76923077, 0.78095238, 0.78787879, 0.77669903, 0.80769231,\n",
       "        0.78787879, 0.77669903, 0.8       , 0.74509804, 0.78095238,\n",
       "        0.78846154, 0.80808081, 0.8       , 0.80769231, 0.78      ,\n",
       "        0.77669903, 0.8       , 0.75247525, 0.78846154, 0.7961165 ,\n",
       "        0.77227723, 0.78095238, 0.77669903, 0.76923077, 0.78846154,\n",
       "        0.78095238, 0.78095238, 0.7961165 , 0.78846154, 0.79245283,\n",
       "        0.78095238, 0.78095238, 0.81132075, 0.78095238, 0.78095238,\n",
       "        0.81132075, 0.80769231, 0.8       , 0.78846154, 0.76923077,\n",
       "        0.75471698, 0.8       , 0.78846154, 0.78095238, 0.82242991,\n",
       "        0.80769231, 0.80392157, 0.78846154, 0.76190476, 0.75471698,\n",
       "        0.8       , 0.78095238, 0.78504673, 0.79245283, 0.80769231,\n",
       "        0.78846154, 0.78095238, 0.75471698, 0.76190476, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.81132075, 0.80769231, 0.80769231,\n",
       "        0.75728155, 0.8       , 0.79245283, 0.75728155, 0.78095238,\n",
       "        0.78846154, 0.75728155, 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.79245283, 0.78095238, 0.79245283, 0.78846154, 0.78846154,\n",
       "        0.78095238, 0.80769231, 0.7961165 , 0.78095238, 0.76190476,\n",
       "        0.77358491, 0.79245283, 0.78095238, 0.78095238, 0.79245283,\n",
       "        0.7961165 , 0.80769231, 0.78846154, 0.76923077, 0.76190476,\n",
       "        0.79245283, 0.78095238, 0.77358491, 0.8       , 0.80769231,\n",
       "        0.80769231, 0.7961165 , 0.76923077, 0.76190476, 0.8       ,\n",
       "        0.76190476, 0.76923077, 0.79245283, 0.80769231, 0.8       ,\n",
       "        0.75728155, 0.80769231, 0.77358491, 0.75728155, 0.79245283,\n",
       "        0.78846154, 0.75728155, 0.79245283, 0.8       , 0.76470588,\n",
       "        0.8       , 0.78504673, 0.75728155, 0.80373832, 0.78846154,\n",
       "        0.75728155, 0.81132075, 0.81132075, 0.76923077, 0.79245283,\n",
       "        0.77358491, 0.76923077, 0.77669903, 0.77358491, 0.76923077,\n",
       "        0.80769231, 0.7961165 , 0.78846154, 0.78095238, 0.76190476,\n",
       "        0.78846154, 0.77669903, 0.75728155, 0.78095238, 0.80769231,\n",
       "        0.80769231, 0.78846154, 0.76190476, 0.76190476, 0.78846154,\n",
       "        0.79245283, 0.76923077, 0.78846154, 0.80769231, 0.8       ,\n",
       "        0.78      , 0.77669903, 0.78504673, 0.76767677, 0.76923077,\n",
       "        0.79245283, 0.75510204, 0.77358491, 0.78504673, 0.78      ,\n",
       "        0.81132075, 0.79245283, 0.77227723, 0.80373832, 0.80373832,\n",
       "        0.77227723, 0.80373832, 0.81132075, 0.78      , 0.80373832,\n",
       "        0.79245283, 0.78      , 0.80373832, 0.78846154, 0.76767677,\n",
       "        0.80373832, 0.81904762, 0.79207921, 0.8       , 0.76923077,\n",
       "        0.79207921, 0.81904762, 0.78095238, 0.76767677, 0.81904762,\n",
       "        0.81904762, 0.78      , 0.78846154, 0.76190476, 0.77227723,\n",
       "        0.8       , 0.76923077, 0.76767677, 0.81904762, 0.80769231,\n",
       "        0.75789474, 0.78      , 0.76923077, 0.75789474, 0.78      ,\n",
       "        0.76923077, 0.75      , 0.78      , 0.76923077, 0.75789474,\n",
       "        0.77227723, 0.76923077, 0.75      , 0.77227723, 0.76923077,\n",
       "        0.74226804, 0.77227723, 0.76923077, 0.76595745, 0.77227723,\n",
       "        0.78095238, 0.75789474, 0.77227723, 0.78095238, 0.75      ,\n",
       "        0.77227723, 0.78095238, 0.75789474, 0.78431373, 0.79245283,\n",
       "        0.75789474, 0.77227723, 0.78846154, 0.7628866 , 0.76      ,\n",
       "        0.78095238, 0.77083333, 0.7961165 , 0.8       , 0.75789474,\n",
       "        0.7961165 , 0.8       , 0.77083333, 0.77227723, 0.79245283]),\n",
       " 'split4_test_score': array([0.7628866 , 0.78      , 0.78      , 0.70707071, 0.7628866 ,\n",
       "        0.75510204, 0.73267327, 0.72727273, 0.7628866 , 0.79207921,\n",
       "        0.78787879, 0.7755102 , 0.74747475, 0.72916667, 0.76767677,\n",
       "        0.74      , 0.73684211, 0.75510204, 0.8       , 0.8       ,\n",
       "        0.79591837, 0.78      , 0.74226804, 0.76767677, 0.76923077,\n",
       "        0.75510204, 0.77227723, 0.76      , 0.80392157, 0.77227723,\n",
       "        0.7961165 , 0.76      , 0.76      , 0.78431373, 0.75510204,\n",
       "        0.77227723, 0.76923077, 0.79207921, 0.76      , 0.7961165 ,\n",
       "        0.76      , 0.75510204, 0.77669903, 0.75510204, 0.77227723,\n",
       "        0.74226804, 0.78      , 0.81188119, 0.73469388, 0.74509804,\n",
       "        0.74747475, 0.72727273, 0.75247525, 0.73469388, 0.7628866 ,\n",
       "        0.80808081, 0.78      , 0.76      , 0.76      , 0.73469388,\n",
       "        0.72164948, 0.72916667, 0.75      , 0.8       , 0.80808081,\n",
       "        0.82      , 0.76767677, 0.75510204, 0.78787879, 0.76767677,\n",
       "        0.7628866 , 0.75510204, 0.77227723, 0.79207921, 0.8       ,\n",
       "        0.77227723, 0.76      , 0.76767677, 0.74747475, 0.75510204,\n",
       "        0.75510204, 0.8       , 0.80392157, 0.79207921, 0.77669903,\n",
       "        0.76      , 0.76767677, 0.78      , 0.75510204, 0.76767677,\n",
       "        0.72164948, 0.80392157, 0.79207921, 0.70833333, 0.77227723,\n",
       "        0.74      , 0.71428571, 0.73267327, 0.72727273, 0.75510204,\n",
       "        0.78787879, 0.80412371, 0.74226804, 0.77227723, 0.75510204,\n",
       "        0.73469388, 0.76      , 0.72916667, 0.75510204, 0.80808081,\n",
       "        0.81632653, 0.74226804, 0.78      , 0.75      , 0.74747475,\n",
       "        0.74226804, 0.75510204, 0.77227723, 0.8       , 0.82      ,\n",
       "        0.75510204, 0.76767677, 0.78787879, 0.73469388, 0.75510204,\n",
       "        0.75510204, 0.77669903, 0.81553398, 0.81188119, 0.76470588,\n",
       "        0.78787879, 0.78      , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.7173913 , 0.76      , 0.78431373, 0.7173913 , 0.74747475,\n",
       "        0.77669903, 0.7173913 , 0.73469388, 0.72727273, 0.72164948,\n",
       "        0.78      , 0.78787879, 0.71428571, 0.74      , 0.76      ,\n",
       "        0.72164948, 0.72727273, 0.73267327, 0.73684211, 0.78787879,\n",
       "        0.80412371, 0.70707071, 0.76470588, 0.78      , 0.71428571,\n",
       "        0.73267327, 0.75510204, 0.74      , 0.79591837, 0.8       ,\n",
       "        0.75247525, 0.77227723, 0.78787879, 0.74      , 0.74      ,\n",
       "        0.75      , 0.76470588, 0.78431373, 0.8       , 0.73267327,\n",
       "        0.8       , 0.78787879, 0.74      , 0.76      , 0.74226804,\n",
       "        0.74157303, 0.75      , 0.74226804, 0.75      , 0.73684211,\n",
       "        0.73469388, 0.74157303, 0.74226804, 0.73469388, 0.72340426,\n",
       "        0.75      , 0.75510204, 0.73684211, 0.74226804, 0.72727273,\n",
       "        0.72916667, 0.75510204, 0.72      , 0.7311828 , 0.75510204,\n",
       "        0.7755102 , 0.74468085, 0.74226804, 0.75510204, 0.72916667,\n",
       "        0.74226804, 0.73469388, 0.75789474, 0.76      , 0.78      ,\n",
       "        0.75      , 0.7628866 , 0.76767677, 0.71578947, 0.75510204,\n",
       "        0.76      , 0.7755102 , 0.78431373, 0.80769231, 0.75      ,\n",
       "        0.74747475, 0.78      , 0.71578947, 0.78787879, 0.76767677,\n",
       "        0.7628866 , 0.78      , 0.78      , 0.70707071, 0.7628866 ,\n",
       "        0.75510204, 0.73267327, 0.72727273, 0.7628866 , 0.79207921,\n",
       "        0.78787879, 0.7755102 , 0.74747475, 0.72916667, 0.76767677,\n",
       "        0.74      , 0.73684211, 0.75510204, 0.8       , 0.8       ,\n",
       "        0.79591837, 0.78      , 0.74226804, 0.76767677, 0.76923077,\n",
       "        0.75510204, 0.77227723, 0.76      , 0.80392157, 0.77227723,\n",
       "        0.7961165 , 0.76      , 0.76      , 0.78431373, 0.75510204,\n",
       "        0.77227723, 0.76923077, 0.79207921, 0.76      , 0.7961165 ,\n",
       "        0.76      , 0.75510204, 0.77669903, 0.75510204, 0.77227723,\n",
       "        0.74226804, 0.78      , 0.81188119, 0.73469388, 0.74509804,\n",
       "        0.74747475, 0.72727273, 0.75247525, 0.73469388, 0.7628866 ,\n",
       "        0.80808081, 0.78      , 0.76      , 0.76      , 0.73469388,\n",
       "        0.72164948, 0.72916667, 0.75      , 0.8       , 0.80808081,\n",
       "        0.82      , 0.76767677, 0.75510204, 0.78787879, 0.76767677,\n",
       "        0.7628866 , 0.75510204, 0.77227723, 0.79207921, 0.8       ,\n",
       "        0.77227723, 0.76      , 0.76767677, 0.74747475, 0.75510204,\n",
       "        0.75510204, 0.8       , 0.80392157, 0.79207921, 0.77669903,\n",
       "        0.76      , 0.76767677, 0.78      , 0.75510204, 0.76767677,\n",
       "        0.72164948, 0.80392157, 0.79207921, 0.70833333, 0.77227723,\n",
       "        0.74      , 0.71428571, 0.73267327, 0.72727273, 0.75510204,\n",
       "        0.78787879, 0.80412371, 0.74226804, 0.77227723, 0.75510204,\n",
       "        0.73469388, 0.76      , 0.72916667, 0.75510204, 0.80808081,\n",
       "        0.81632653, 0.74226804, 0.78      , 0.75      , 0.74747475,\n",
       "        0.74226804, 0.75510204, 0.77227723, 0.8       , 0.82      ,\n",
       "        0.75510204, 0.76767677, 0.78787879, 0.73469388, 0.75510204,\n",
       "        0.75510204, 0.77669903, 0.81553398, 0.81188119, 0.76470588,\n",
       "        0.78787879, 0.78      , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.7173913 , 0.76      , 0.78431373, 0.7173913 , 0.74747475,\n",
       "        0.77669903, 0.7173913 , 0.73469388, 0.72727273, 0.72164948,\n",
       "        0.78      , 0.78787879, 0.71428571, 0.74      , 0.76      ,\n",
       "        0.72164948, 0.72727273, 0.73267327, 0.73684211, 0.78787879,\n",
       "        0.80412371, 0.70707071, 0.76470588, 0.78      , 0.71428571,\n",
       "        0.73267327, 0.75510204, 0.74      , 0.79591837, 0.8       ,\n",
       "        0.75247525, 0.77227723, 0.78787879, 0.74      , 0.74      ,\n",
       "        0.75      , 0.76470588, 0.78431373, 0.8       , 0.73267327,\n",
       "        0.8       , 0.78787879, 0.74      , 0.76      , 0.74226804,\n",
       "        0.74157303, 0.75      , 0.74226804, 0.75      , 0.73684211,\n",
       "        0.73469388, 0.74157303, 0.74226804, 0.73469388, 0.72340426,\n",
       "        0.75      , 0.75510204, 0.73684211, 0.74226804, 0.72727273,\n",
       "        0.72916667, 0.75510204, 0.72      , 0.7311828 , 0.75510204,\n",
       "        0.7755102 , 0.74468085, 0.74226804, 0.75510204, 0.72916667,\n",
       "        0.74226804, 0.73469388, 0.75789474, 0.76      , 0.78      ,\n",
       "        0.75      , 0.7628866 , 0.76767677, 0.71578947, 0.75510204,\n",
       "        0.76      , 0.7755102 , 0.78431373, 0.80769231, 0.75      ,\n",
       "        0.74747475, 0.78      , 0.71578947, 0.78787879, 0.76767677,\n",
       "        0.75510204, 0.8       , 0.79207921, 0.72      , 0.72164948,\n",
       "        0.76      , 0.7       , 0.73267327, 0.73267327, 0.76190476,\n",
       "        0.80808081, 0.7755102 , 0.73584906, 0.73469388, 0.75510204,\n",
       "        0.70588235, 0.72727273, 0.73469388, 0.79207921, 0.81632653,\n",
       "        0.8       , 0.7254902 , 0.73469388, 0.7755102 , 0.73786408,\n",
       "        0.73267327, 0.78431373, 0.76190476, 0.81188119, 0.7755102 ,\n",
       "        0.77358491, 0.76470588, 0.76470588, 0.73786408, 0.74      ,\n",
       "        0.76470588, 0.78095238, 0.8       , 0.76767677, 0.76923077,\n",
       "        0.74509804, 0.76470588, 0.76190476, 0.73267327, 0.76470588,\n",
       "        0.73469388, 0.78787879, 0.82      , 0.72727273, 0.78      ,\n",
       "        0.71428571, 0.71428571, 0.72727273, 0.74      , 0.74747475,\n",
       "        0.78350515, 0.79591837, 0.72      , 0.74509804, 0.74      ,\n",
       "        0.7254902 , 0.7       , 0.74226804, 0.76923077, 0.81632653,\n",
       "        0.82828283, 0.74509804, 0.76      , 0.76      , 0.73786408,\n",
       "        0.71428571, 0.73469388, 0.76923077, 0.8       , 0.82      ,\n",
       "        0.76635514, 0.77227723, 0.79207921, 0.76190476, 0.7254902 ,\n",
       "        0.73267327, 0.79245283, 0.7961165 , 0.79207921, 0.78504673,\n",
       "        0.77669903, 0.76470588, 0.76190476, 0.74      , 0.73267327,\n",
       "        0.72340426, 0.77669903, 0.78431373, 0.72916667, 0.78846154,\n",
       "        0.73469388, 0.70833333, 0.72      , 0.75247525, 0.72      ,\n",
       "        0.78      , 0.79166667, 0.71428571, 0.73267327, 0.74226804,\n",
       "        0.70707071, 0.7254902 , 0.72727273, 0.73267327, 0.77227723,\n",
       "        0.80412371, 0.72727273, 0.75247525, 0.78      , 0.72      ,\n",
       "        0.7254902 , 0.72727273, 0.75728155, 0.80392157, 0.82      ,\n",
       "        0.75247525, 0.78846154, 0.78      , 0.73267327, 0.74      ,\n",
       "        0.74226804, 0.78846154, 0.78846154, 0.81188119, 0.77669903,\n",
       "        0.75728155, 0.75728155, 0.74509804, 0.74      , 0.76767677,\n",
       "        0.73913043, 0.74747475, 0.78431373, 0.72527473, 0.73267327,\n",
       "        0.78095238, 0.7311828 , 0.74747475, 0.74509804, 0.72916667,\n",
       "        0.74      , 0.78      , 0.72727273, 0.76923077, 0.76470588,\n",
       "        0.72727273, 0.7254902 , 0.73786408, 0.74747475, 0.75728155,\n",
       "        0.78787879, 0.74747475, 0.75728155, 0.76470588, 0.74      ,\n",
       "        0.7254902 , 0.71287129, 0.74747475, 0.78846154, 0.80392157,\n",
       "        0.73469388, 0.77358491, 0.76923077, 0.73469388, 0.75      ,\n",
       "        0.74509804, 0.72164948, 0.76923077, 0.7961165 , 0.75      ,\n",
       "        0.78095238, 0.76470588, 0.73469388, 0.75      , 0.75247525,\n",
       "        0.66666667, 0.72527473, 0.72164948, 0.68131868, 0.73913043,\n",
       "        0.71428571, 0.65168539, 0.7311828 , 0.72164948, 0.68131868,\n",
       "        0.72164948, 0.72727273, 0.68131868, 0.72164948, 0.72      ,\n",
       "        0.69565217, 0.71428571, 0.74509804, 0.7173913 , 0.72727273,\n",
       "        0.74      , 0.68888889, 0.72727273, 0.7254902 , 0.68888889,\n",
       "        0.72727273, 0.73267327, 0.7173913 , 0.76767677, 0.75      ,\n",
       "        0.7173913 , 0.72727273, 0.75247525, 0.7032967 , 0.73469388,\n",
       "        0.73267327, 0.69565217, 0.76      , 0.76923077, 0.7173913 ,\n",
       "        0.73469388, 0.75728155, 0.7173913 , 0.72727273, 0.74      ]),\n",
       " 'mean_test_score': array([0.7745352 , 0.77681357, 0.76486632, 0.75789874, 0.77305351,\n",
       "        0.76485689, 0.76526141, 0.77388959, 0.78173776, 0.77623167,\n",
       "        0.77464002, 0.75290853, 0.76683371, 0.76127289, 0.77165952,\n",
       "        0.76783897, 0.7719959 , 0.78400777, 0.77952848, 0.7759518 ,\n",
       "        0.76583415, 0.78029367, 0.77073601, 0.78043242, 0.77440395,\n",
       "        0.77046241, 0.7762959 , 0.76749935, 0.77179607, 0.75590629,\n",
       "        0.78197851, 0.77566334, 0.77164136, 0.7805197 , 0.77055445,\n",
       "        0.77705914, 0.77654625, 0.77313298, 0.75576154, 0.78569045,\n",
       "        0.77569308, 0.7712521 , 0.77553523, 0.7734484 , 0.76913388,\n",
       "        0.76634931, 0.77504762, 0.7802381 , 0.76388442, 0.77430043,\n",
       "        0.7702825 , 0.75528109, 0.77593861, 0.77016876, 0.76795241,\n",
       "        0.78018013, 0.76812205, 0.77634937, 0.76992008, 0.76327744,\n",
       "        0.76067499, 0.77285342, 0.7791231 , 0.77340263, 0.77234347,\n",
       "        0.76732862, 0.77518009, 0.77223628, 0.77285579, 0.76405781,\n",
       "        0.77179308, 0.77494753, 0.77253673, 0.76926898, 0.77048566,\n",
       "        0.77403675, 0.77282886, 0.77954301, 0.77050095, 0.77113697,\n",
       "        0.77062733, 0.7773007 , 0.77571649, 0.76920566, 0.77717239,\n",
       "        0.77102051, 0.78380756, 0.77426923, 0.78153492, 0.77667532,\n",
       "        0.7550506 , 0.77209384, 0.77883282, 0.75301282, 0.77435084,\n",
       "        0.77401413, 0.75586867, 0.77053142, 0.77404429, 0.76280564,\n",
       "        0.78545359, 0.77644246, 0.76513839, 0.78053011, 0.7776987 ,\n",
       "        0.75949763, 0.7753624 , 0.77634784, 0.75785647, 0.78096353,\n",
       "        0.76966589, 0.76529417, 0.77824002, 0.76222854, 0.76326919,\n",
       "        0.77308314, 0.77259746, 0.76080166, 0.77243697, 0.77886559,\n",
       "        0.77036812, 0.77085508, 0.78044242, 0.7702829 , 0.77187847,\n",
       "        0.77116717, 0.7678688 , 0.77594021, 0.78091721, 0.77213495,\n",
       "        0.77319406, 0.77860211, 0.76195757, 0.77089853, 0.77121583,\n",
       "        0.74598866, 0.77211657, 0.772938  , 0.74968371, 0.77120841,\n",
       "        0.76854005, 0.74974046, 0.76700921, 0.76813589, 0.74551814,\n",
       "        0.77314335, 0.77950826, 0.7468524 , 0.7714018 , 0.7727436 ,\n",
       "        0.74348484, 0.76479112, 0.76881514, 0.75351944, 0.77217067,\n",
       "        0.78197797, 0.74643978, 0.77339127, 0.7771727 , 0.74779889,\n",
       "        0.76587122, 0.77830089, 0.75054003, 0.77631198, 0.77230597,\n",
       "        0.76016502, 0.77495866, 0.77197297, 0.75433034, 0.77167056,\n",
       "        0.76781224, 0.7608315 , 0.77232971, 0.77130932, 0.75752519,\n",
       "        0.77790692, 0.78338299, 0.75521954, 0.77502306, 0.76863781,\n",
       "        0.7276625 , 0.74994116, 0.75457022, 0.72725763, 0.75040346,\n",
       "        0.76534659, 0.72344382, 0.75230333, 0.75761181, 0.73928536,\n",
       "        0.7539139 , 0.76228432, 0.73869183, 0.75019171, 0.76511978,\n",
       "        0.72030873, 0.75239174, 0.75584528, 0.74362128, 0.75884594,\n",
       "        0.76858947, 0.74626598, 0.75499241, 0.76342746, 0.72735931,\n",
       "        0.75723747, 0.76324252, 0.74739799, 0.75649475, 0.76933358,\n",
       "        0.74834699, 0.76083128, 0.76723562, 0.72428768, 0.76069228,\n",
       "        0.76998643, 0.76144488, 0.76890356, 0.77370158, 0.75008361,\n",
       "        0.75577186, 0.76693711, 0.7280501 , 0.77181088, 0.7716242 ,\n",
       "        0.7745352 , 0.77681357, 0.76486632, 0.75789874, 0.77305351,\n",
       "        0.76485689, 0.76526141, 0.77388959, 0.78173776, 0.77623167,\n",
       "        0.77464002, 0.75290853, 0.76683371, 0.76127289, 0.77165952,\n",
       "        0.76783897, 0.7719959 , 0.78400777, 0.77952848, 0.7759518 ,\n",
       "        0.76583415, 0.78029367, 0.77073601, 0.78043242, 0.77440395,\n",
       "        0.77046241, 0.7762959 , 0.76749935, 0.77179607, 0.75590629,\n",
       "        0.78197851, 0.77566334, 0.77164136, 0.7805197 , 0.77055445,\n",
       "        0.77705914, 0.77654625, 0.77313298, 0.75576154, 0.78569045,\n",
       "        0.77569308, 0.7712521 , 0.77553523, 0.7734484 , 0.76913388,\n",
       "        0.76634931, 0.77504762, 0.7802381 , 0.76388442, 0.77430043,\n",
       "        0.7702825 , 0.75528109, 0.77593861, 0.77016876, 0.76795241,\n",
       "        0.78018013, 0.76812205, 0.77634937, 0.76992008, 0.76327744,\n",
       "        0.76067499, 0.77285342, 0.7791231 , 0.77340263, 0.77234347,\n",
       "        0.76732862, 0.77518009, 0.77223628, 0.77285579, 0.76405781,\n",
       "        0.77179308, 0.77494753, 0.77253673, 0.76926898, 0.77048566,\n",
       "        0.77403675, 0.77282886, 0.77954301, 0.77050095, 0.77113697,\n",
       "        0.77062733, 0.7773007 , 0.77571649, 0.76920566, 0.77717239,\n",
       "        0.77102051, 0.78380756, 0.77426923, 0.78153492, 0.77667532,\n",
       "        0.7550506 , 0.77209384, 0.77883282, 0.75301282, 0.77435084,\n",
       "        0.77401413, 0.75586867, 0.77053142, 0.77404429, 0.76280564,\n",
       "        0.78545359, 0.77644246, 0.76513839, 0.78053011, 0.7776987 ,\n",
       "        0.75949763, 0.7753624 , 0.77634784, 0.75785647, 0.78096353,\n",
       "        0.76966589, 0.76529417, 0.77824002, 0.76222854, 0.76326919,\n",
       "        0.77308314, 0.77259746, 0.76080166, 0.77243697, 0.77886559,\n",
       "        0.77036812, 0.77085508, 0.78044242, 0.7702829 , 0.77187847,\n",
       "        0.77116717, 0.7678688 , 0.77594021, 0.78091721, 0.77213495,\n",
       "        0.77319406, 0.77860211, 0.76195757, 0.77089853, 0.77121583,\n",
       "        0.74598866, 0.77211657, 0.772938  , 0.74968371, 0.77120841,\n",
       "        0.76854005, 0.74974046, 0.76700921, 0.76813589, 0.74551814,\n",
       "        0.77314335, 0.77950826, 0.7468524 , 0.7714018 , 0.7727436 ,\n",
       "        0.74348484, 0.76479112, 0.76881514, 0.75351944, 0.77217067,\n",
       "        0.78197797, 0.74643978, 0.77339127, 0.7771727 , 0.74779889,\n",
       "        0.76587122, 0.77830089, 0.75054003, 0.77631198, 0.77230597,\n",
       "        0.76016502, 0.77495866, 0.77197297, 0.75433034, 0.77167056,\n",
       "        0.76781224, 0.7608315 , 0.77232971, 0.77130932, 0.75752519,\n",
       "        0.77790692, 0.78338299, 0.75521954, 0.77502306, 0.76863781,\n",
       "        0.7276625 , 0.74994116, 0.75457022, 0.72725763, 0.75040346,\n",
       "        0.76534659, 0.72344382, 0.75230333, 0.75761181, 0.73928536,\n",
       "        0.7539139 , 0.76228432, 0.73869183, 0.75019171, 0.76511978,\n",
       "        0.72030873, 0.75239174, 0.75584528, 0.74362128, 0.75884594,\n",
       "        0.76858947, 0.74626598, 0.75499241, 0.76342746, 0.72735931,\n",
       "        0.75723747, 0.76324252, 0.74739799, 0.75649475, 0.76933358,\n",
       "        0.74834699, 0.76083128, 0.76723562, 0.72428768, 0.76069228,\n",
       "        0.76998643, 0.76144488, 0.76890356, 0.77370158, 0.75008361,\n",
       "        0.75577186, 0.76693711, 0.7280501 , 0.77181088, 0.7716242 ,\n",
       "        0.76551745, 0.78236002, 0.76762806, 0.75892613, 0.76914675,\n",
       "        0.77383883, 0.75708374, 0.77288722, 0.77303658, 0.76252536,\n",
       "        0.77142626, 0.76370984, 0.77164015, 0.77290947, 0.77044221,\n",
       "        0.76394384, 0.77066032, 0.77186018, 0.77276521, 0.77973754,\n",
       "        0.76429347, 0.76560125, 0.7558981 , 0.77019479, 0.77602799,\n",
       "        0.76992555, 0.77941263, 0.76163392, 0.76637781, 0.74396904,\n",
       "        0.77355062, 0.77196512, 0.77250494, 0.7665666 , 0.77058109,\n",
       "        0.77679365, 0.76326113, 0.7696262 , 0.7504773 , 0.77660136,\n",
       "        0.76538425, 0.77101876, 0.77204806, 0.76966142, 0.7806398 ,\n",
       "        0.75923004, 0.78602367, 0.79119985, 0.75764906, 0.77019886,\n",
       "        0.76348375, 0.75421089, 0.77316443, 0.77613356, 0.76455144,\n",
       "        0.78189872, 0.77459049, 0.76385003, 0.7709615 , 0.77295421,\n",
       "        0.76495101, 0.7696003 , 0.77259831, 0.76669053, 0.77350892,\n",
       "        0.77665059, 0.77117267, 0.77352122, 0.76757062, 0.77121074,\n",
       "        0.76891664, 0.77528074, 0.75638619, 0.77494503, 0.77345285,\n",
       "        0.77166475, 0.76930033, 0.76868393, 0.77315646, 0.77476837,\n",
       "        0.77660458, 0.77047577, 0.77385414, 0.7607797 , 0.77837298,\n",
       "        0.77245715, 0.77401915, 0.76973079, 0.77606063, 0.77247627,\n",
       "        0.7519032 , 0.78158156, 0.78273495, 0.75551113, 0.78777951,\n",
       "        0.77517457, 0.75326112, 0.76909193, 0.776919  , 0.75525609,\n",
       "        0.7779637 , 0.77706828, 0.75627994, 0.77708573, 0.77467738,\n",
       "        0.75253295, 0.7706453 , 0.77559896, 0.75141132, 0.77642986,\n",
       "        0.77379151, 0.75685382, 0.77283518, 0.77049401, 0.7581361 ,\n",
       "        0.76752977, 0.7692294 , 0.75367881, 0.77850357, 0.77532952,\n",
       "        0.7587996 , 0.7777757 , 0.76516716, 0.76550597, 0.77446459,\n",
       "        0.77400778, 0.76561178, 0.773974  , 0.76994138, 0.76742296,\n",
       "        0.7755172 , 0.76156594, 0.76058733, 0.77340212, 0.78047675,\n",
       "        0.75893286, 0.7733722 , 0.77306587, 0.75278768, 0.76533947,\n",
       "        0.77889256, 0.74936898, 0.76674821, 0.77095508, 0.74118799,\n",
       "        0.76484167, 0.77431172, 0.74368067, 0.77894319, 0.7802902 ,\n",
       "        0.74332506, 0.76929009, 0.77418634, 0.74226279, 0.77174276,\n",
       "        0.78241588, 0.74635683, 0.77562398, 0.77630548, 0.74595116,\n",
       "        0.77005722, 0.77130991, 0.7497139 , 0.76626526, 0.7783614 ,\n",
       "        0.75150481, 0.78315343, 0.77345664, 0.75127822, 0.77324648,\n",
       "        0.77340605, 0.7449102 , 0.76177478, 0.77463647, 0.75875717,\n",
       "        0.77914782, 0.77565046, 0.75137449, 0.77324648, 0.77385142,\n",
       "        0.70836799, 0.75058329, 0.74451105, 0.71000563, 0.75335443,\n",
       "        0.75253993, 0.69795457, 0.75423656, 0.75936227, 0.70260269,\n",
       "        0.73961544, 0.74575451, 0.69918771, 0.74016254, 0.7514676 ,\n",
       "        0.70884268, 0.73895007, 0.76401312, 0.72564056, 0.74428259,\n",
       "        0.75435549, 0.71476098, 0.74596341, 0.75960343, 0.70999169,\n",
       "        0.74227539, 0.75992161, 0.71847322, 0.76020226, 0.75718014,\n",
       "        0.72199682, 0.74197855, 0.76226983, 0.72035957, 0.74524881,\n",
       "        0.76137792, 0.71981247, 0.7555959 , 0.7601991 , 0.71627102,\n",
       "        0.75428524, 0.76415019, 0.72786671, 0.74767634, 0.76514335]),\n",
       " 'std_test_score': array([0.03603841, 0.03889515, 0.05118018, 0.05079308, 0.04737488,\n",
       "        0.04270123, 0.05184125, 0.05308768, 0.05074934, 0.05335622,\n",
       "        0.05049767, 0.04262064, 0.05588253, 0.05355784, 0.03758682,\n",
       "        0.0529106 , 0.04245371, 0.04355105, 0.04251393, 0.04212135,\n",
       "        0.04124022, 0.051517  , 0.0423233 , 0.04118831, 0.03861494,\n",
       "        0.03891457, 0.03688882, 0.0400447 , 0.03815112, 0.04085513,\n",
       "        0.05407964, 0.04047694, 0.0480125 , 0.0442267 , 0.04431704,\n",
       "        0.03808745, 0.03670267, 0.04222239, 0.03681787, 0.04188036,\n",
       "        0.03965489, 0.03958213, 0.04412225, 0.044157  , 0.04066513,\n",
       "        0.05283885, 0.04801511, 0.05224304, 0.0547872 , 0.04948926,\n",
       "        0.05028811, 0.04346709, 0.0569654 , 0.05615554, 0.04686602,\n",
       "        0.04831998, 0.04677124, 0.04938741, 0.0421951 , 0.04170848,\n",
       "        0.04770955, 0.052908  , 0.04674607, 0.04328596, 0.05072569,\n",
       "        0.04454351, 0.04586779, 0.03930686, 0.03351793, 0.04301996,\n",
       "        0.03946553, 0.04617025, 0.04550883, 0.03072291, 0.03455915,\n",
       "        0.04809943, 0.04552048, 0.03980375, 0.04957263, 0.04819846,\n",
       "        0.03972143, 0.0393947 , 0.04780726, 0.04492008, 0.0463901 ,\n",
       "        0.04656584, 0.04092109, 0.05373125, 0.04705288, 0.0454655 ,\n",
       "        0.05110163, 0.05117014, 0.05348996, 0.05322931, 0.04068323,\n",
       "        0.04459046, 0.05565319, 0.05171949, 0.05575918, 0.03970056,\n",
       "        0.04490673, 0.04971567, 0.04474175, 0.04573973, 0.04121935,\n",
       "        0.04147015, 0.03872762, 0.05094914, 0.04441987, 0.03732773,\n",
       "        0.0478596 , 0.03962056, 0.04177388, 0.03029295, 0.0504339 ,\n",
       "        0.05413799, 0.04611775, 0.0472983 , 0.04500158, 0.05072913,\n",
       "        0.04273051, 0.03636976, 0.04499359, 0.05518764, 0.05222449,\n",
       "        0.03779355, 0.05293462, 0.03666617, 0.0409877 , 0.0405318 ,\n",
       "        0.04063496, 0.04081377, 0.05334428, 0.04448017, 0.04141847,\n",
       "        0.05074608, 0.0463109 , 0.04907735, 0.05099555, 0.05048523,\n",
       "        0.04846242, 0.05214868, 0.04924794, 0.05403858, 0.05530944,\n",
       "        0.04402706, 0.04657744, 0.05154059, 0.04719041, 0.04707483,\n",
       "        0.04209597, 0.04594618, 0.04776709, 0.04530388, 0.04241423,\n",
       "        0.05045449, 0.0549605 , 0.04905706, 0.03954774, 0.0539337 ,\n",
       "        0.04510732, 0.04931096, 0.05040752, 0.05282912, 0.05561226,\n",
       "        0.05586783, 0.05015737, 0.04434168, 0.04956748, 0.04728778,\n",
       "        0.05059951, 0.0503893 , 0.04260455, 0.03081779, 0.05307103,\n",
       "        0.05690506, 0.03871942, 0.05365418, 0.04527183, 0.05036289,\n",
       "        0.0591367 , 0.05124847, 0.0409421 , 0.05960397, 0.0521128 ,\n",
       "        0.04427511, 0.05479651, 0.04927186, 0.0472461 , 0.05255644,\n",
       "        0.04625151, 0.04129577, 0.05200877, 0.05048459, 0.04832176,\n",
       "        0.04847282, 0.04189317, 0.0452153 , 0.03943804, 0.04809579,\n",
       "        0.04832386, 0.04868823, 0.04263853, 0.0523682 , 0.04695823,\n",
       "        0.04189756, 0.04317728, 0.04570451, 0.0421207 , 0.04567981,\n",
       "        0.04946913, 0.04860663, 0.05301927, 0.04159209, 0.04235271,\n",
       "        0.04470495, 0.04269643, 0.04345031, 0.05110791, 0.04766945,\n",
       "        0.0551336 , 0.05763957, 0.04375146, 0.04383065, 0.0459627 ,\n",
       "        0.03603841, 0.03889515, 0.05118018, 0.05079308, 0.04737488,\n",
       "        0.04270123, 0.05184125, 0.05308768, 0.05074934, 0.05335622,\n",
       "        0.05049767, 0.04262064, 0.05588253, 0.05355784, 0.03758682,\n",
       "        0.0529106 , 0.04245371, 0.04355105, 0.04251393, 0.04212135,\n",
       "        0.04124022, 0.051517  , 0.0423233 , 0.04118831, 0.03861494,\n",
       "        0.03891457, 0.03688882, 0.0400447 , 0.03815112, 0.04085513,\n",
       "        0.05407964, 0.04047694, 0.0480125 , 0.0442267 , 0.04431704,\n",
       "        0.03808745, 0.03670267, 0.04222239, 0.03681787, 0.04188036,\n",
       "        0.03965489, 0.03958213, 0.04412225, 0.044157  , 0.04066513,\n",
       "        0.05283885, 0.04801511, 0.05224304, 0.0547872 , 0.04948926,\n",
       "        0.05028811, 0.04346709, 0.0569654 , 0.05615554, 0.04686602,\n",
       "        0.04831998, 0.04677124, 0.04938741, 0.0421951 , 0.04170848,\n",
       "        0.04770955, 0.052908  , 0.04674607, 0.04328596, 0.05072569,\n",
       "        0.04454351, 0.04586779, 0.03930686, 0.03351793, 0.04301996,\n",
       "        0.03946553, 0.04617025, 0.04550883, 0.03072291, 0.03455915,\n",
       "        0.04809943, 0.04552048, 0.03980375, 0.04957263, 0.04819846,\n",
       "        0.03972143, 0.0393947 , 0.04780726, 0.04492008, 0.0463901 ,\n",
       "        0.04656584, 0.04092109, 0.05373125, 0.04705288, 0.0454655 ,\n",
       "        0.05110163, 0.05117014, 0.05348996, 0.05322931, 0.04068323,\n",
       "        0.04459046, 0.05565319, 0.05171949, 0.05575918, 0.03970056,\n",
       "        0.04490673, 0.04971567, 0.04474175, 0.04573973, 0.04121935,\n",
       "        0.04147015, 0.03872762, 0.05094914, 0.04441987, 0.03732773,\n",
       "        0.0478596 , 0.03962056, 0.04177388, 0.03029295, 0.0504339 ,\n",
       "        0.05413799, 0.04611775, 0.0472983 , 0.04500158, 0.05072913,\n",
       "        0.04273051, 0.03636976, 0.04499359, 0.05518764, 0.05222449,\n",
       "        0.03779355, 0.05293462, 0.03666617, 0.0409877 , 0.0405318 ,\n",
       "        0.04063496, 0.04081377, 0.05334428, 0.04448017, 0.04141847,\n",
       "        0.05074608, 0.0463109 , 0.04907735, 0.05099555, 0.05048523,\n",
       "        0.04846242, 0.05214868, 0.04924794, 0.05403858, 0.05530944,\n",
       "        0.04402706, 0.04657744, 0.05154059, 0.04719041, 0.04707483,\n",
       "        0.04209597, 0.04594618, 0.04776709, 0.04530388, 0.04241423,\n",
       "        0.05045449, 0.0549605 , 0.04905706, 0.03954774, 0.0539337 ,\n",
       "        0.04510732, 0.04931096, 0.05040752, 0.05282912, 0.05561226,\n",
       "        0.05586783, 0.05015737, 0.04434168, 0.04956748, 0.04728778,\n",
       "        0.05059951, 0.0503893 , 0.04260455, 0.03081779, 0.05307103,\n",
       "        0.05690506, 0.03871942, 0.05365418, 0.04527183, 0.05036289,\n",
       "        0.0591367 , 0.05124847, 0.0409421 , 0.05960397, 0.0521128 ,\n",
       "        0.04427511, 0.05479651, 0.04927186, 0.0472461 , 0.05255644,\n",
       "        0.04625151, 0.04129577, 0.05200877, 0.05048459, 0.04832176,\n",
       "        0.04847282, 0.04189317, 0.0452153 , 0.03943804, 0.04809579,\n",
       "        0.04832386, 0.04868823, 0.04263853, 0.0523682 , 0.04695823,\n",
       "        0.04189756, 0.04317728, 0.04570451, 0.0421207 , 0.04567981,\n",
       "        0.04946913, 0.04860663, 0.05301927, 0.04159209, 0.04235271,\n",
       "        0.04470495, 0.04269643, 0.04345031, 0.05110791, 0.04766945,\n",
       "        0.0551336 , 0.05763957, 0.04375146, 0.04383065, 0.0459627 ,\n",
       "        0.04435273, 0.03924802, 0.047024  , 0.05322084, 0.0559312 ,\n",
       "        0.04996245, 0.05459616, 0.05637238, 0.05868041, 0.05203002,\n",
       "        0.05027311, 0.04582115, 0.05677495, 0.05201356, 0.04827569,\n",
       "        0.05904516, 0.05661696, 0.0512165 , 0.05239409, 0.04772642,\n",
       "        0.04386804, 0.05400174, 0.05607686, 0.03711502, 0.06212152,\n",
       "        0.05368164, 0.03665607, 0.04333658, 0.05060194, 0.03427513,\n",
       "        0.04765629, 0.04149652, 0.04175097, 0.05152566, 0.04752902,\n",
       "        0.03711082, 0.04877451, 0.03429945, 0.02929777, 0.05374214,\n",
       "        0.04809681, 0.04373216, 0.05156675, 0.05060928, 0.03906581,\n",
       "        0.05567635, 0.04510191, 0.04419554, 0.05421948, 0.04798465,\n",
       "        0.05185959, 0.05863693, 0.05804944, 0.05011897, 0.04717305,\n",
       "        0.04281145, 0.04729184, 0.05271315, 0.06223977, 0.04660135,\n",
       "        0.05475784, 0.0611963 , 0.04980724, 0.0581867 , 0.04143484,\n",
       "        0.04492938, 0.05317114, 0.04150106, 0.04003033, 0.05369223,\n",
       "        0.06015207, 0.04520005, 0.05054098, 0.0384963 , 0.05027464,\n",
       "        0.04862132, 0.04414095, 0.04998294, 0.05515495, 0.05288988,\n",
       "        0.04521396, 0.05122232, 0.03849775, 0.03558477, 0.04719856,\n",
       "        0.04479962, 0.04647342, 0.04873345, 0.04645494, 0.04575006,\n",
       "        0.05620118, 0.04195139, 0.04947251, 0.05618199, 0.04709584,\n",
       "        0.05426715, 0.05578124, 0.05719962, 0.05646619, 0.05153631,\n",
       "        0.04801147, 0.0467476 , 0.05991602, 0.05889179, 0.04153582,\n",
       "        0.04982942, 0.05337261, 0.05355755, 0.05135982, 0.04786871,\n",
       "        0.05319124, 0.04598338, 0.04484709, 0.04571869, 0.04912104,\n",
       "        0.05308337, 0.05303421, 0.05394964, 0.04320333, 0.05263217,\n",
       "        0.05390568, 0.05173857, 0.04920944, 0.05651601, 0.05309706,\n",
       "        0.05389917, 0.06265363, 0.04061477, 0.04157563, 0.05697461,\n",
       "        0.04410184, 0.05081586, 0.05002092, 0.0515172 , 0.04848471,\n",
       "        0.05164955, 0.03917976, 0.03922546, 0.05890778, 0.05037266,\n",
       "        0.05243823, 0.05495926, 0.05000795, 0.05043291, 0.05858016,\n",
       "        0.05604895, 0.05233323, 0.05231974, 0.04926686, 0.05290716,\n",
       "        0.04791703, 0.0539286 , 0.05300877, 0.04383193, 0.0592833 ,\n",
       "        0.04897411, 0.04564332, 0.04846907, 0.05047029, 0.03717271,\n",
       "        0.057688  , 0.05922199, 0.05072906, 0.05392754, 0.04779937,\n",
       "        0.04561485, 0.04675694, 0.04618478, 0.04434691, 0.05236241,\n",
       "        0.05581043, 0.05758782, 0.05379246, 0.04097139, 0.05124544,\n",
       "        0.04168542, 0.04788891, 0.04692485, 0.05236241, 0.04954863,\n",
       "        0.05901273, 0.06228366, 0.05116516, 0.06610298, 0.06139791,\n",
       "        0.04958118, 0.06429917, 0.05505131, 0.05363469, 0.05693881,\n",
       "        0.05823286, 0.05261086, 0.06578952, 0.05799193, 0.0564671 ,\n",
       "        0.06201839, 0.04777669, 0.04500277, 0.05755265, 0.05413284,\n",
       "        0.05401094, 0.055446  , 0.05376053, 0.05176771, 0.0519456 ,\n",
       "        0.04277084, 0.04556489, 0.06055649, 0.05797804, 0.05510914,\n",
       "        0.05513723, 0.05028167, 0.05229744, 0.05318106, 0.04347809,\n",
       "        0.04534032, 0.05769833, 0.05634292, 0.05332577, 0.05137736,\n",
       "        0.06676898, 0.05569348, 0.05210626, 0.04531919, 0.04565825]),\n",
       " 'rank_test_score': array([159,  92, 439, 514, 216, 441, 430, 181,  23, 114, 155, 571, 407,\n",
       "        483, 279, 389, 261,   8,  52, 119, 418,  42, 313,  40, 162, 330,\n",
       "        112, 396, 271, 532,  18, 129, 281,  35, 320,  89, 100, 211, 541,\n",
       "          4, 127, 292, 134, 194, 366, 413, 144,  45, 453, 167, 337, 545,\n",
       "        123, 341, 385,  47, 383, 105, 348, 460, 494, 226,  58, 197, 244,\n",
       "        399, 141, 250, 224, 449, 273, 150, 237, 360, 327, 174, 229,  50,\n",
       "        324, 302, 317,  81, 125, 363,  85, 304,  10, 169,  26,  95, 550,\n",
       "        258,  64, 569, 164, 177, 535, 322, 172, 467,   6, 102, 434,  33,\n",
       "         79, 503, 137, 107, 516,  28, 351, 428,  73, 473, 462, 213, 235,\n",
       "        489, 242,  62, 333, 311,  38, 335, 266, 300, 387, 121,  30, 254,\n",
       "        205,  66, 475, 309, 294, 618, 256, 220, 601, 297, 379, 598, 403,\n",
       "        381, 623, 209,  54, 611, 287, 232, 633, 444, 372, 565, 252,  20,\n",
       "        613, 200,  83, 606, 416,  71, 587, 109, 248, 499, 148, 263, 557,\n",
       "        276, 391, 485, 246, 290, 521,  76,  12, 548, 146, 375, 650, 596,\n",
       "        554, 654, 590, 425, 659, 578, 519, 642, 562, 470, 645, 592, 436,\n",
       "        663, 576, 537, 631, 509, 377, 616, 552, 458, 652, 523, 465, 609,\n",
       "        528, 356, 604, 487, 401, 657, 492, 344, 480, 370, 187, 594, 539,\n",
       "        405, 647, 269, 284, 159,  92, 439, 514, 216, 441, 430, 181,  23,\n",
       "        114, 155, 571, 407, 483, 279, 389, 261,   8,  52, 119, 418,  42,\n",
       "        313,  40, 162, 330, 112, 396, 271, 532,  18, 129, 281,  35, 320,\n",
       "         89, 100, 211, 541,   4, 127, 292, 134, 194, 366, 413, 144,  45,\n",
       "        453, 167, 337, 545, 123, 341, 385,  47, 383, 105, 348, 460, 494,\n",
       "        226,  58, 197, 244, 399, 141, 250, 224, 449, 273, 150, 237, 360,\n",
       "        327, 174, 229,  50, 324, 302, 317,  81, 125, 363,  85, 304,  10,\n",
       "        169,  26,  95, 550, 258,  64, 569, 164, 177, 535, 322, 172, 467,\n",
       "          6, 102, 434,  33,  79, 503, 137, 107, 516,  28, 351, 428,  73,\n",
       "        473, 462, 213, 235, 489, 242,  62, 333, 311,  38, 335, 266, 300,\n",
       "        387, 121,  30, 254, 205,  66, 475, 309, 294, 618, 256, 220, 601,\n",
       "        297, 379, 598, 403, 381, 623, 209,  54, 611, 287, 232, 633, 444,\n",
       "        372, 565, 252,  20, 613, 200,  83, 606, 416,  71, 587, 109, 248,\n",
       "        499, 148, 263, 557, 276, 391, 485, 246, 290, 521,  76,  12, 548,\n",
       "        146, 375, 650, 596, 554, 654, 590, 425, 659, 578, 519, 642, 562,\n",
       "        470, 645, 592, 436, 663, 576, 537, 631, 509, 377, 616, 552, 458,\n",
       "        652, 523, 465, 609, 528, 356, 604, 487, 401, 657, 492, 344, 480,\n",
       "        370, 187, 594, 539, 405, 647, 269, 284, 422,  17, 393, 508, 365,\n",
       "        185, 526, 223, 218, 469, 286, 456, 283, 222, 332, 452, 315, 268,\n",
       "        231,  49, 447, 421, 534, 340, 118, 347,  56, 478, 412, 629, 189,\n",
       "        265, 239, 411, 319,  94, 464, 354, 589,  99, 424, 306, 260, 353,\n",
       "         32, 506,   3,   1, 518, 339, 457, 561, 207, 116, 446,  22, 158,\n",
       "        455, 307, 219, 438, 355, 234, 410, 191,  97, 299, 190, 394, 296,\n",
       "        369, 140, 530, 152, 193, 278, 358, 374, 208, 153,  98, 329, 183,\n",
       "        491,  69, 241, 176, 350, 117, 240, 580,  25,  15, 544,   2, 143,\n",
       "        568, 368,  91, 547,  75,  88, 531,  87, 154, 575, 316, 133, 583,\n",
       "        104, 186, 527, 228, 326, 513, 395, 362, 564,  68, 139, 511,  78,\n",
       "        432, 423, 161, 179, 420, 180, 346, 398, 136, 479, 496, 199,  37,\n",
       "        507, 202, 215, 573, 427,  61, 603, 409, 308, 639, 443, 166, 630,\n",
       "         60,  44, 635, 359, 171, 637, 275,  16, 615, 132, 111, 621, 343,\n",
       "        289, 600, 415,  70, 581,  14, 192, 585, 203, 196, 626, 477, 157,\n",
       "        512,  57, 131, 584, 203, 184, 672, 586, 627, 669, 567, 574, 675,\n",
       "        560, 505, 673, 641, 622, 674, 640, 582, 671, 644, 451, 656, 628,\n",
       "        556, 668, 620, 502, 670, 636, 501, 666, 497, 525, 661, 638, 472,\n",
       "        662, 625, 482, 665, 543, 498, 667, 559, 448, 649, 608, 433])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.4,\n",
       " 'learning_rate': 0.07,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7911998502810007"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.789238\n",
      "F1: 0.675862\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': 42,\n",
       " 'SibSp': 90,\n",
       " 'Parch': 67,\n",
       " 'Q': 32,\n",
       " 'S': 33,\n",
       " 'Pclass': 82,\n",
       " 'Age': 108,\n",
       " 'Fare': 123,\n",
       " 'youngin': 25}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=3, missing=nan, n_estimators=500, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
